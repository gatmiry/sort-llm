{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NlbPLmYwHhe",
        "outputId": "fafa2d27-a30a-4bde-917e-99798e912543"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnathan-henry\u001b[0m (\u001b[33mnathan-henry-uc-berkeley-electrical-engineering-computer\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Drive mount skipped/failed: Error: credential propagation was unsuccessful\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# wandb + (optional) Google Drive for downloading saves of models to it\n",
        "# =========================\n",
        "\n",
        "# SET GoogleDrive DIR TO SAVE TO FOR THE EXPERIMENT!!\n",
        "# ---------------------------------------------------\n",
        "# ---------------------------------------------------\n",
        "GDRIVE_BASE_DIR = \"/content/drive/MyDrive/sort_with_duplicate_mixing_2\"\n",
        "# ---------------------------------------------------\n",
        "# ---------------------------------------------------\n",
        "# ---------------------------------------------------\n",
        "\n",
        "!pip -q install wandb\n",
        "\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# ---- W&B ----\n",
        "wandb.login()\n",
        "\n",
        "# ---- Google Drive (optional) ----\n",
        "# Set this to False if you want to skip Drive entirely.\n",
        "ENABLE_GDRIVE = True\n",
        "\n",
        "# Globals used by Cell 2 + Cell 3\n",
        "GDRIVE_MOUNTED = False\n",
        "GDRIVE_MODEL_DIR = None  # destination where .pt files get copied as they are saved\n",
        "\n",
        "if ENABLE_GDRIVE:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\")\n",
        "        GDRIVE_MOUNTED = True\n",
        "\n",
        "        # Set the base folder ONCE here. Cell 3 can optionally set a per-grid subfolder.\n",
        "        os.makedirs(GDRIVE_BASE_DIR, exist_ok=True)\n",
        "\n",
        "        # Default: copy into base; Cell 3 may overwrite to a per-grid folder.\n",
        "        GDRIVE_MODEL_DIR = GDRIVE_BASE_DIR\n",
        "\n",
        "        print(\"‚úÖ Drive mounted.\")\n",
        "        print(f\"   GDRIVE_BASE_DIR  = {GDRIVE_BASE_DIR}\")\n",
        "        print(f\"   GDRIVE_MODEL_DIR = {GDRIVE_MODEL_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Drive mount skipped/failed: {e}\")\n",
        "        GDRIVE_MOUNTED = False\n",
        "else:\n",
        "    print(\"Drive mount disabled (ENABLE_GDRIVE=False).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8JvsirmnfRv",
        "outputId": "61099b0b-04c4-4356-ddd7-75dbce576cd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# setup (model, batching, training, grid runner)\n",
        "# =========================\n",
        "import os, time, copy, math, shutil\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Tuple, Iterable, Dict, Any\n",
        "from fractions import Fraction\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "import wandb\n",
        "\n",
        "# Optional (for pretty summary grid)\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "# -------------------------\n",
        "# Speed knobs\n",
        "# -------------------------\n",
        "def enable_tf32():\n",
        "    if torch.cuda.is_available():\n",
        "        if hasattr(torch.backends.cuda.matmul, \"fp32_precision\"):\n",
        "            torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
        "        else:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "        if (\n",
        "            hasattr(torch.backends, \"cudnn\")\n",
        "            and hasattr(torch.backends.cudnn, \"conv\")\n",
        "            and hasattr(torch.backends.cudnn.conv, \"fp32_precision\")\n",
        "        ):\n",
        "            torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
        "        else:\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "enable_tf32()\n",
        "\n",
        "# -------------------------\n",
        "# Console clear (Jupyter + terminal-safe)\n",
        "# -------------------------\n",
        "def clear_console():\n",
        "    try:\n",
        "        from IPython.display import clear_output\n",
        "        clear_output(wait=True)\n",
        "        return\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"\\033[2J\\033[H\", end=\"\")\n",
        "\n",
        "# -------------------------\n",
        "# Robust context helpers (NO device_type kwarg)\n",
        "# -------------------------\n",
        "def get_sdpa_context():\n",
        "    try:\n",
        "        from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "        return sdpa_kernel([SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH])\n",
        "    except Exception:\n",
        "        return nullcontext()\n",
        "\n",
        "def get_autocast_context(device: torch.device, dtype: Optional[torch.dtype]):\n",
        "    if device.type != \"cuda\" or dtype is None:\n",
        "        return nullcontext()\n",
        "    try:\n",
        "        return torch.amp.autocast(\"cuda\", dtype=dtype)\n",
        "    except Exception:\n",
        "        return torch.cuda.amp.autocast(dtype=dtype)\n",
        "\n",
        "def make_grad_scaler(enabled: bool):\n",
        "    if not enabled:\n",
        "        class _NoScaler:\n",
        "            def is_enabled(self): return False\n",
        "            def scale(self, x): return x\n",
        "            def step(self, opt): opt.step()\n",
        "            def update(self): pass\n",
        "        return _NoScaler()\n",
        "    try:\n",
        "        return torch.amp.GradScaler()\n",
        "    except Exception:\n",
        "        return torch.cuda.amp.GradScaler()\n",
        "\n",
        "# -------------------------\n",
        "# Model components\n",
        "# -------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.fc_2 = nn.Linear(3 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc_2(self.gelu(self.fc_1(x)))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_heads == 0\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.n_embd // config.n_heads\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.use_mlp = bool(getattr(config, \"use_mlp\", True))\n",
        "\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        if self.use_mlp:\n",
        "            self.mlp = MLP(config)\n",
        "            self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        else:\n",
        "            self.mlp = None\n",
        "            self.ln_2 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        if self.mlp is not None:\n",
        "            x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_size: int,\n",
        "        vocab_size: int,\n",
        "        n_layers: int = 2,\n",
        "        n_heads: int = 1,\n",
        "        n_embd: int = 64,\n",
        "        without_pos: bool = False,\n",
        "        use_mlp: bool = True,\n",
        "        max_seq_len: Optional[int] = None,\n",
        "    ):\n",
        "        self.block_size = int(block_size)\n",
        "        self.vocab_size = int(vocab_size)\n",
        "        self.n_layers = int(n_layers)\n",
        "        self.n_heads = int(n_heads)\n",
        "        self.n_embd = int(n_embd)\n",
        "        self.without_pos = bool(without_pos)\n",
        "        self.use_mlp = bool(use_mlp)\n",
        "        self.max_seq_len = int(max_seq_len if max_seq_len is not None else (2 * self.block_size + 1))\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_layers = config.n_layers\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.max_seq_len, config.n_embd),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.transformer.wte.weight  # weight tying\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.register_buffer(\"pos_idx\", torch.arange(config.max_seq_len), persistent=False)\n",
        "\n",
        "        # Positional encodings fixed to 0 (and frozen) if without_pos=True\n",
        "        if self.config.without_pos:\n",
        "            with torch.no_grad():\n",
        "                self.transformer.wpe.weight.zero_()\n",
        "            self.transformer.wpe.weight.requires_grad_(False)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
        "                std *= (2 * self.n_layers) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx, return_full_logits: bool = False, block_size: Optional[int] = None):\n",
        "        B, T = idx.size()\n",
        "        if block_size is None:\n",
        "            block_size = self.config.block_size\n",
        "        block_size = int(block_size)\n",
        "\n",
        "        expected_T = 2 * block_size + 1\n",
        "        assert T == expected_T, f\"Expected T={expected_T} for block_size={block_size}, got T={T}\"\n",
        "        assert T <= self.config.max_seq_len, f\"T={T} exceeds max_seq_len={self.config.max_seq_len}\"\n",
        "\n",
        "        pos = self.transformer.wpe(self.pos_idx[:T])\n",
        "        x = self.transformer.wte(idx) if self.config.without_pos else (self.transformer.wte(idx) + pos)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        targets = idx[:, block_size + 1 :]  # (B, K)\n",
        "\n",
        "        if return_full_logits:\n",
        "            logits = self.lm_head(x)                       # (B, T, V)\n",
        "            logits_for_loss = logits[:, block_size:T-1, :] # (B, K, V)\n",
        "        else:\n",
        "            x_for_loss = x[:, block_size:T-1, :]           # (B, K, C)\n",
        "            logits_for_loss = self.lm_head(x_for_loss)     # (B, K, V)\n",
        "            logits = logits_for_loss\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits_for_loss.reshape(-1, logits_for_loss.size(-1)),\n",
        "            targets.reshape(-1),\n",
        "        )\n",
        "        return logits, loss\n",
        "\n",
        "# -------------------------\n",
        "# Ratio + embedding dim helpers\n",
        "# -------------------------\n",
        "def compute_effective_n_embd(vocab_n: int, n_heads: int, vocab_over_embd: Optional[float], fallback_n_embd: int) -> int:\n",
        "    \"\"\"\n",
        "    Implements: (vocab_size / embd_dim) = r  => embd_dim = vocab_size / r\n",
        "    vocab_size includes SEP => total_vocab = vocab_n + 1\n",
        "    Then rounds up to be divisible by n_heads.\n",
        "    \"\"\"\n",
        "    if vocab_over_embd is None:\n",
        "        n_embd = int(fallback_n_embd)\n",
        "    else:\n",
        "        r = float(vocab_over_embd)\n",
        "        if r <= 0:\n",
        "            raise ValueError(f\"vocab_over_embd must be > 0, got {vocab_over_embd}\")\n",
        "        total_vocab_size = int(vocab_n) + 1\n",
        "        n_embd = int(round(total_vocab_size / r))\n",
        "        n_embd = max(1, n_embd)\n",
        "\n",
        "    n_heads = int(n_heads)\n",
        "    if n_embd % n_heads != 0:\n",
        "        n_embd = ((n_embd + n_heads - 1) // n_heads) * n_heads\n",
        "    return int(n_embd)\n",
        "\n",
        "# -------------------------\n",
        "# Data batching (p-way duplicate mixing, controlled by with_mixing boolean)\n",
        "# -------------------------\n",
        "SEP_TOKEN = \"SEP\"\n",
        "\n",
        "def _sample_no_duplicates(batch_size: int, vocab_n: int, block_size: int, device: torch.device) -> torch.Tensor:\n",
        "    if block_size > vocab_n:\n",
        "        raise ValueError(\n",
        "            f\"Cannot sample {block_size} unique tokens from vocab_n={vocab_n}. \"\n",
        "            f\"Need block_size <= vocab_n for 'no-duplicates' sampling.\"\n",
        "        )\n",
        "    scores = torch.rand(batch_size, vocab_n, device=device)\n",
        "    return scores.topk(block_size, dim=1).indices.to(torch.long)\n",
        "\n",
        "def _sample_with_duplicates(batch_size: int, vocab_n: int, block_size: int, device: torch.device) -> torch.Tensor:\n",
        "    return torch.randint(0, vocab_n, (batch_size, block_size), device=device, dtype=torch.long)\n",
        "\n",
        "def _sample_from_random_subset_with_replacement(\n",
        "    batch_size: int,\n",
        "    vocab_n: int,\n",
        "    block_size: int,\n",
        "    device: torch.device,\n",
        "    effective_vocab_n: int,\n",
        ") -> torch.Tensor:\n",
        "    effective_vocab_n = int(effective_vocab_n)\n",
        "    if not (1 <= effective_vocab_n <= vocab_n):\n",
        "        raise ValueError(f\"effective_vocab_n must be in [1, {vocab_n}], got {effective_vocab_n}\")\n",
        "\n",
        "    if effective_vocab_n == vocab_n:\n",
        "        return _sample_with_duplicates(batch_size, vocab_n, block_size, device)\n",
        "\n",
        "    scores = torch.rand(batch_size, vocab_n, device=device)\n",
        "    allowed = scores.topk(effective_vocab_n, dim=1).indices  # (B, effective_vocab_n)\n",
        "\n",
        "    pick = torch.randint(0, effective_vocab_n, (batch_size, block_size), device=device)\n",
        "    return allowed.gather(1, pick).to(torch.long)\n",
        "\n",
        "# rotation offset so remainder distribution is balanced over time\n",
        "_DUP_MIX_OFFSET = 0\n",
        "\n",
        "def _sample_numbers_mixed_duplicate_curriculum(\n",
        "    batch_size: int,\n",
        "    vocab_n: int,\n",
        "    block_size: int,\n",
        "    device: torch.device,\n",
        "    p: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Even split across t=0..p-1 (as evenly as possible):\n",
        "      t=0: guaranteed 0 duplicates (no replacement, full vocab)\n",
        "      t>0: remove floor(t/p * vocab_n) items (per-sample), then sample with replacement\n",
        "    \"\"\"\n",
        "    global _DUP_MIX_OFFSET\n",
        "    p = int(p)\n",
        "    if p <= 0:\n",
        "        raise ValueError(f\"p must be >= 1, got {p}\")\n",
        "    if p == 1:\n",
        "        return _sample_with_duplicates(batch_size, vocab_n, block_size, device)\n",
        "\n",
        "    base = batch_size // p\n",
        "    rem = batch_size % p\n",
        "\n",
        "    # distribute remainder in a rotating way across calls\n",
        "    extra = [0] * p\n",
        "    start = int(_DUP_MIX_OFFSET % p)\n",
        "    for i in range(rem):\n",
        "        extra[(start + i) % p] += 1\n",
        "    _DUP_MIX_OFFSET += rem\n",
        "\n",
        "    groups = []\n",
        "    for t in range(p):\n",
        "        n_t = base + extra[t]\n",
        "        if n_t <= 0:\n",
        "            continue\n",
        "\n",
        "        if t == 0:\n",
        "            x_t = _sample_no_duplicates(n_t, vocab_n, block_size, device)\n",
        "        else:\n",
        "            n_removed = (vocab_n * t) // p\n",
        "            eff_vocab = vocab_n - n_removed  # >= 1 since t < p\n",
        "            x_t = _sample_from_random_subset_with_replacement(n_t, vocab_n, block_size, device, eff_vocab)\n",
        "\n",
        "        groups.append(x_t)\n",
        "\n",
        "    x = torch.cat(groups, dim=0)\n",
        "    perm = torch.randperm(x.size(0), device=device)\n",
        "    return x[perm]\n",
        "\n",
        "def _sample_numbers(\n",
        "    batch_size: int,\n",
        "    vocab_n: int,\n",
        "    block_size: int,\n",
        "    device: torch.device,\n",
        "    allow_duplicates: bool,\n",
        "    *,\n",
        "    dup_mixture_p: Optional[int] = None,\n",
        "    use_dup_mixture: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    if not allow_duplicates:\n",
        "        return _sample_no_duplicates(batch_size, vocab_n, block_size, device)\n",
        "\n",
        "    # allow_duplicates=True\n",
        "    if use_dup_mixture and (dup_mixture_p is not None) and int(dup_mixture_p) > 1:\n",
        "        return _sample_numbers_mixed_duplicate_curriculum(batch_size, vocab_n, block_size, device, int(dup_mixture_p))\n",
        "\n",
        "    return _sample_with_duplicates(batch_size, vocab_n, block_size, device)\n",
        "\n",
        "def _build_batch_from_x(x: torch.Tensor, vocab_n: int) -> torch.Tensor:\n",
        "    vals = x.sort(dim=1).values\n",
        "    sep_id = vocab_n\n",
        "    sep = torch.full((x.size(0), 1), sep_id, device=x.device, dtype=torch.long)\n",
        "    return torch.cat([x, sep, vals], dim=1)\n",
        "\n",
        "def get_batch(\n",
        "    batch_size: int,\n",
        "    device: torch.device,\n",
        "    vocab_n: int,\n",
        "    block_size: int,\n",
        "    allow_duplicates: bool,\n",
        "    *,\n",
        "    dup_mixture_p: Optional[int] = None,\n",
        "    use_dup_mixture: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    x = _sample_numbers(\n",
        "        batch_size=batch_size,\n",
        "        vocab_n=vocab_n,\n",
        "        block_size=block_size,\n",
        "        device=device,\n",
        "        allow_duplicates=allow_duplicates,\n",
        "        dup_mixture_p=dup_mixture_p,\n",
        "        use_dup_mixture=use_dup_mixture,\n",
        "    )\n",
        "    return _build_batch_from_x(x, vocab_n)\n",
        "\n",
        "# -------------------------\n",
        "# LR + plateau helpers\n",
        "# -------------------------\n",
        "def create_optimizer(model, weight_decay: float, lr: float):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    decay_params = [p for p in params if p.dim() > 1]\n",
        "    nondecay_params = [p for p in params if p.dim() <= 1]\n",
        "    optim_groups = [\n",
        "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "        {\"params\": nondecay_params, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    try:\n",
        "        return torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=True)\n",
        "    except TypeError:\n",
        "        return torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "def get_lr(itr: int, cfg) -> float:\n",
        "    if itr < cfg.warmup_iters:\n",
        "        return cfg.learning_rate * (itr + 1) / (cfg.warmup_iters + 1)\n",
        "    if itr > cfg.max_iters:\n",
        "        return cfg.min_lr\n",
        "    ratio = (itr - cfg.warmup_iters) / (cfg.max_iters - cfg.warmup_iters)\n",
        "    ratio = 0.5 * (1.0 + math.cos(math.pi * ratio))\n",
        "    return cfg.min_lr + ratio * (cfg.learning_rate - cfg.min_lr)\n",
        "\n",
        "def _safe_log10(x: float, eps: float = 1e-30) -> float:\n",
        "    return math.log10(max(float(x), eps))\n",
        "\n",
        "def _linreg_slope(xs, ys) -> float:\n",
        "    n = len(xs)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "    x_mean = sum(xs) / n\n",
        "    y_mean = sum(ys) / n\n",
        "    cov = 0.0\n",
        "    var = 0.0\n",
        "    for x, y in zip(xs, ys):\n",
        "        dx = x - x_mean\n",
        "        dy = y - y_mean\n",
        "        cov += dx * dy\n",
        "        var += dx * dx\n",
        "    return cov / var if var > 0 else 0.0\n",
        "\n",
        "# -------------------------\n",
        "# Accuracy helpers\n",
        "# -------------------------\n",
        "def acc_from_logits(logits: torch.Tensor, idx: torch.Tensor, block_size: int):\n",
        "    targets = idx[:, block_size + 1 :]\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    token_acc = (preds == targets).float().mean()\n",
        "    sample_acc = (preds == targets).all(dim=1).float().mean()\n",
        "    return token_acc, sample_acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_batch_metrics(model: nn.Module, device: torch.device, cfg, block_size: int, use_amp: bool, amp_dtype: Optional[torch.dtype], batch_size: Optional[int] = None):\n",
        "    model.eval()\n",
        "    bs = int(batch_size) if batch_size is not None else int(cfg.micro_batch_size)\n",
        "\n",
        "    batch = get_batch(\n",
        "        batch_size=bs,\n",
        "        device=device,\n",
        "        vocab_n=cfg.vocab_n,\n",
        "        block_size=int(block_size),\n",
        "        allow_duplicates=cfg.allow_duplicates,\n",
        "        # Eval stays \"original\" distribution (no mixing) by default\n",
        "    )\n",
        "\n",
        "    with (get_autocast_context(device, amp_dtype) if use_amp else nullcontext()):\n",
        "        logits, loss = model(batch, return_full_logits=False, block_size=int(block_size))\n",
        "\n",
        "    token_acc_t, sample_acc_t = acc_from_logits(logits, batch, int(block_size))\n",
        "    model.train()\n",
        "    return float(loss.item()), float(token_acc_t.item()), float(sample_acc_t.item())\n",
        "\n",
        "# -------------------------\n",
        "# NEW: evaluation by exact number of unique tokens in the input list\n",
        "# -------------------------\n",
        "def _sample_exact_unique_count(\n",
        "    batch_size: int,\n",
        "    vocab_n: int,\n",
        "    block_size: int,\n",
        "    num_unique: int,\n",
        "    device: torch.device,\n",
        ") -> torch.Tensor:\n",
        "    num_unique = int(num_unique)\n",
        "    if not (1 <= num_unique <= min(block_size, vocab_n)):\n",
        "        raise ValueError(f\"num_unique must be in [1, min(K,vocab_n)] got {num_unique} (K={block_size}, vocab_n={vocab_n})\")\n",
        "\n",
        "    scores = torch.rand(batch_size, vocab_n, device=device)\n",
        "    uniq = scores.topk(num_unique, dim=1).indices.to(torch.long)  # (B, U)\n",
        "\n",
        "    if block_size == num_unique:\n",
        "        x = uniq\n",
        "    else:\n",
        "        extra_idx = torch.randint(0, num_unique, (batch_size, block_size - num_unique), device=device)\n",
        "        extra = uniq.gather(1, extra_idx)\n",
        "        x = torch.cat([uniq, extra], dim=1)\n",
        "        perm = torch.rand(batch_size, block_size, device=device).argsort(dim=1)\n",
        "        x = x.gather(1, perm)\n",
        "\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_sample_acc_by_unique_count(\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    cfg,\n",
        "    block_size: int,\n",
        "    use_amp: bool,\n",
        "    amp_dtype: Optional[torch.dtype],\n",
        "    batch_size: Optional[int] = None,\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    k = int(block_size)\n",
        "    vmax = min(k, int(cfg.vocab_n))\n",
        "    bs = int(batch_size) if batch_size is not None else int(cfg.micro_batch_size)\n",
        "\n",
        "    base = bs // vmax\n",
        "    rem = bs % vmax\n",
        "\n",
        "    xs = []\n",
        "    v_labels = []\n",
        "    for i, V in enumerate(range(1, vmax + 1)):\n",
        "        n_v = base + (1 if i < rem else 0)\n",
        "        if n_v <= 0:\n",
        "            continue\n",
        "        x_v = _sample_exact_unique_count(n_v, cfg.vocab_n, k, V, device)\n",
        "        xs.append(x_v)\n",
        "        v_labels.append(torch.full((n_v,), V, device=device, dtype=torch.long))\n",
        "\n",
        "    x_all = torch.cat(xs, dim=0)\n",
        "    v_all = torch.cat(v_labels, dim=0)\n",
        "\n",
        "    perm = torch.randperm(x_all.size(0), device=device)\n",
        "    x_all = x_all[perm]\n",
        "    v_all = v_all[perm]\n",
        "\n",
        "    batch = _build_batch_from_x(x_all, cfg.vocab_n)\n",
        "\n",
        "    with (get_autocast_context(device, amp_dtype) if use_amp else nullcontext()):\n",
        "        logits, _ = model(batch, return_full_logits=False, block_size=k)\n",
        "\n",
        "    targets = batch[:, k + 1 :]\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    sample_correct = (preds == targets).all(dim=1)\n",
        "\n",
        "    out = {}\n",
        "    for V in range(1, vmax + 1):\n",
        "        mask = (v_all == V)\n",
        "        out[V] = float(sample_correct[mask].float().mean().item()) if mask.any() else float(\"nan\")\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Config (NOTE: with_mixing boolean replaces \"p\" hyperparam)\n",
        "# with_mixing=False => effective p=1\n",
        "# with_mixing=True  => effective p=mixing_bins (default 8)\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    vocab_n: int = 1024\n",
        "    block_size: int = 32\n",
        "    allow_duplicates: bool = True\n",
        "    sep_token: str = SEP_TOKEN\n",
        "\n",
        "    # NEW: boolean toggle for mixing\n",
        "    with_mixing: bool = False\n",
        "    mixing_bins: int = 8  # when with_mixing=True, effective p = mixing_bins; when False, effective p=1\n",
        "\n",
        "    # NEW: log sample accuracy by exact number of unique tokens V in the input\n",
        "    log_unique_count_metrics: bool = True\n",
        "    unique_count_eval_batch_size: Optional[int] = None  # None => use micro_batch_size\n",
        "\n",
        "    test_block_sizes: Optional[Tuple[int, ...]] = None\n",
        "\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 1\n",
        "    n_embd: int = 64\n",
        "    vocab_over_embd: Optional[float] = None\n",
        "\n",
        "    without_pos: bool = False\n",
        "    use_mlp: bool = True\n",
        "\n",
        "    warmup_iters: int = 200\n",
        "    max_iters: int = 120000\n",
        "    learning_rate: float = 1e-4\n",
        "    min_lr: float = 1e-6\n",
        "    weight_decay: float = 0.0\n",
        "\n",
        "    micro_batch_size: int = 1024\n",
        "    effective_batch_size: int = 4096\n",
        "\n",
        "    log_interval: int = 250\n",
        "    ckpt_interval: int = 20000\n",
        "    save_dir: str = \"./saved_models\"\n",
        "\n",
        "    plateau_window_logs: int = 40\n",
        "    plateau_slope_threshold: float = 0.02\n",
        "    plateau_log10_loss_delta: float = 0.02\n",
        "    plateau_patience_logs: int = 2\n",
        "    plateau_extra_logs: int = 1\n",
        "    plateau_min_iters: int = 20000\n",
        "\n",
        "    seed: int = 1337\n",
        "    use_compile: bool = False\n",
        "\n",
        "    wandb_project: str = \"sortgpt\"\n",
        "    wandb_entity: Optional[str] = None\n",
        "    wandb_group: Optional[str] = None\n",
        "    wandb_mode: Optional[str] = None\n",
        "\n",
        "def _mix_p_effective(cfg: TrainConfig) -> int:\n",
        "    return int(cfg.mixing_bins) if bool(cfg.with_mixing) else 1\n",
        "\n",
        "# -------------------------\n",
        "# Naming / saving + Drive copy hook (NEW: interpretable names)\n",
        "# -------------------------\n",
        "def make_wandb_run_name(cfg: TrainConfig) -> str:\n",
        "    eff_n_embd = compute_effective_n_embd(cfg.vocab_n, cfg.n_heads, cfg.vocab_over_embd, cfg.n_embd)\n",
        "    return (\n",
        "        f\"vocab{int(cfg.vocab_n)}\"\n",
        "        f\"_blockSize{int(cfg.block_size)}\"\n",
        "        f\"_layers{int(cfg.n_layers)}\"\n",
        "        f\"_pos{int(cfg.without_pos)}\"\n",
        "        f\"_mlp{int(cfg.use_mlp)}\"\n",
        "        f\"_embd{int(eff_n_embd)}\"\n",
        "        f\"_dup{int(cfg.allow_duplicates)}\"\n",
        "        f\"_mix{int(cfg.with_mixing)}\"\n",
        "    )\n",
        "\n",
        "def make_save_filename(prefix: str, cfg: TrainConfig, iters_done: int) -> str:\n",
        "    eff_n_embd = compute_effective_n_embd(cfg.vocab_n, cfg.n_heads, cfg.vocab_over_embd, cfg.n_embd)\n",
        "    return (\n",
        "        f\"{prefix}\"\n",
        "        f\"_vocab{int(cfg.vocab_n)}\"\n",
        "        f\"_blockSize{int(cfg.block_size)}\"\n",
        "        f\"_layers{int(cfg.n_layers)}\"\n",
        "        f\"_pos{int(cfg.without_pos)}\"\n",
        "        f\"_mlp{int(cfg.use_mlp)}\"\n",
        "        f\"_embd{int(eff_n_embd)}\"\n",
        "        f\"_dup{int(cfg.allow_duplicates)}\"\n",
        "        f\"_mix{int(cfg.with_mixing)}\"\n",
        "        f\"_iters{int(iters_done)}.pt\"\n",
        "    )\n",
        "\n",
        "def maybe_copy_to_drive(local_path: str):\n",
        "    \"\"\"\n",
        "    Copies local_path -> GDRIVE_MODEL_DIR if Drive was mounted in Cell 1.\n",
        "    If Drive setup was skipped, this becomes a no-op.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        mounted = bool(globals().get(\"GDRIVE_MOUNTED\", False))\n",
        "        dst_dir = globals().get(\"GDRIVE_MODEL_DIR\", None)\n",
        "        if (not mounted) or (not dst_dir):\n",
        "            return\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "        dst_path = os.path.join(dst_dir, os.path.basename(local_path))\n",
        "        shutil.copy2(local_path, dst_path)\n",
        "        print(f\"‚úì Copied to Drive: {dst_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Drive copy failed for {local_path}: {e}\")\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "def train_sorting_gpt(cfg: TrainConfig) -> Dict[str, Any]:\n",
        "    t0 = time.time()\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "    test_block_sizes = tuple(int(k) for k in (cfg.test_block_sizes if cfg.test_block_sizes else (cfg.block_size,)))\n",
        "    primary_test_k = int(max(test_block_sizes))\n",
        "\n",
        "    # Validate strict no-duplicates mode\n",
        "    if not cfg.allow_duplicates:\n",
        "        for k in (int(cfg.block_size),) + tuple(test_block_sizes):\n",
        "            if k > cfg.vocab_n:\n",
        "                raise ValueError(f\"allow_duplicates=False requires block_size <= vocab_n (got block_size={k}, vocab_n={cfg.vocab_n})\")\n",
        "\n",
        "    total_vocab_size = cfg.vocab_n + 1\n",
        "    sep_id = cfg.vocab_n\n",
        "\n",
        "    grad_accum_steps = cfg.effective_batch_size // cfg.micro_batch_size\n",
        "    assert cfg.effective_batch_size % cfg.micro_batch_size == 0\n",
        "\n",
        "    # AMP\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    if use_amp:\n",
        "        bf16_ok = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
        "        amp_dtype = torch.bfloat16 if bf16_ok else torch.float16\n",
        "    else:\n",
        "        amp_dtype = None\n",
        "    scaler = make_grad_scaler(enabled=(use_amp and amp_dtype == torch.float16))\n",
        "\n",
        "    # Effective embd + max_seq_len for largest tested K\n",
        "    eff_n_embd = compute_effective_n_embd(cfg.vocab_n, cfg.n_heads, cfg.vocab_over_embd, cfg.n_embd)\n",
        "    max_k_for_model = max([int(cfg.block_size)] + [int(k) for k in test_block_sizes])\n",
        "    max_seq_len = 2 * max_k_for_model + 1\n",
        "\n",
        "    model_cfg = GPTConfig(\n",
        "        block_size=cfg.block_size,\n",
        "        vocab_size=total_vocab_size,\n",
        "        n_layers=cfg.n_layers,\n",
        "        n_heads=cfg.n_heads,\n",
        "        n_embd=eff_n_embd,\n",
        "        without_pos=cfg.without_pos,\n",
        "        use_mlp=cfg.use_mlp,\n",
        "        max_seq_len=max_seq_len,\n",
        "    )\n",
        "    model = GPT(model_cfg).to(device)\n",
        "\n",
        "    if cfg.use_compile and hasattr(torch, \"compile\"):\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"max-autotune\")\n",
        "            print(\"torch.compile enabled\")\n",
        "        except Exception as e:\n",
        "            print(f\"torch.compile failed, continuing uncompiled: {e}\")\n",
        "\n",
        "    optimizer = create_optimizer(model, weight_decay=cfg.weight_decay, lr=cfg.learning_rate)\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # W&B init\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "\n",
        "    mix_p_eff = _mix_p_effective(cfg)\n",
        "\n",
        "    wandb_cfg = asdict(cfg)\n",
        "    wandb_cfg.update(\n",
        "        dict(\n",
        "            total_vocab_size=total_vocab_size,\n",
        "            sep_id=sep_id,\n",
        "            max_seq_len=max_seq_len,\n",
        "            max_k_for_model=max_k_for_model,\n",
        "            grad_accum_steps=grad_accum_steps,\n",
        "            amp_dtype=str(amp_dtype) if amp_dtype is not None else \"none\",\n",
        "            device=str(device),\n",
        "            test_block_sizes=list(test_block_sizes),\n",
        "            primary_test_k=int(primary_test_k),\n",
        "            n_embd_effective=int(eff_n_embd),\n",
        "            mix_bins_effective=int(mix_p_eff),  # 1 if mix disabled, else 8 by default\n",
        "        )\n",
        "    )\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=cfg.wandb_project,\n",
        "        entity=cfg.wandb_entity,\n",
        "        group=cfg.wandb_group,\n",
        "        name=make_wandb_run_name(cfg),\n",
        "        config=wandb_cfg,\n",
        "        mode=cfg.wandb_mode,\n",
        "    )\n",
        "\n",
        "    run.define_metric(\"iter\")\n",
        "    run.define_metric(\"train/*\", step_metric=\"iter\")\n",
        "    run.define_metric(\"test/*\", step_metric=\"iter\")\n",
        "    run.define_metric(\"lr\", step_metric=\"iter\")\n",
        "    run.define_metric(\"plateau/*\", step_metric=\"iter\")\n",
        "    run.define_metric(\"ll/iter\")\n",
        "    run.define_metric(\"ll/*\", step_metric=\"ll/iter\")\n",
        "\n",
        "    # Plateau state\n",
        "    ll_iters_hist, ll_loss_hist = [], []\n",
        "    plateau_hits = 0\n",
        "    plateau_reached = False\n",
        "    plateau_reached_iter = None\n",
        "    stop_after_log_iter = None\n",
        "\n",
        "    thresholds = [0.9, 0.99, 0.999, 0.9999, 1.0]\n",
        "    first_iter_at = {thr: None for thr in thresholds}\n",
        "\n",
        "    last_test_metrics = {}\n",
        "    iters_done = cfg.max_iters\n",
        "    last_log_t = time.time()\n",
        "\n",
        "    with get_sdpa_context():\n",
        "        for itr in trange(cfg.max_iters, desc=\"training\"):\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss_accum = torch.zeros((), device=device)\n",
        "\n",
        "            do_log = (itr % cfg.log_interval == 0)\n",
        "            if do_log:\n",
        "                token_correct = torch.zeros((), device=device)\n",
        "                sample_correct = torch.zeros((), device=device)\n",
        "                token_total = 0\n",
        "                sample_total = 0\n",
        "\n",
        "            for _ in range(grad_accum_steps):\n",
        "                # TRAINING BATCH uses mixing if cfg.with_mixing=True\n",
        "                mix_p_eff = _mix_p_effective(cfg)\n",
        "                batch = get_batch(\n",
        "                    batch_size=cfg.micro_batch_size,\n",
        "                    device=device,\n",
        "                    vocab_n=cfg.vocab_n,\n",
        "                    block_size=cfg.block_size,\n",
        "                    allow_duplicates=cfg.allow_duplicates,\n",
        "                    dup_mixture_p=mix_p_eff,\n",
        "                    use_dup_mixture=True,\n",
        "                )\n",
        "\n",
        "                if use_amp:\n",
        "                    with get_autocast_context(device, amp_dtype):\n",
        "                        logits, loss = model(batch, return_full_logits=False, block_size=cfg.block_size)\n",
        "                else:\n",
        "                    logits, loss = model(batch, return_full_logits=False, block_size=cfg.block_size)\n",
        "\n",
        "                if do_log:\n",
        "                    with torch.no_grad():\n",
        "                        targets = batch[:, cfg.block_size + 1 :]\n",
        "                        preds = logits.detach().argmax(dim=-1)\n",
        "                        token_correct += (preds == targets).sum()\n",
        "                        sample_correct += (preds == targets).all(dim=1).sum()\n",
        "                        token_total += targets.numel()\n",
        "                        sample_total += targets.size(0)\n",
        "\n",
        "                loss_to_back = loss / grad_accum_steps\n",
        "                if scaler.is_enabled():\n",
        "                    scaler.scale(loss_to_back).backward()\n",
        "                else:\n",
        "                    loss_to_back.backward()\n",
        "\n",
        "                loss_accum += loss.detach()\n",
        "\n",
        "            lr = get_lr(itr, cfg)\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = lr\n",
        "\n",
        "            if scaler.is_enabled():\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "            # ---- log + plateau check ----\n",
        "            if do_log:\n",
        "                train_loss = float((loss_accum / grad_accum_steps).item())\n",
        "                train_token_acc = float((token_correct / max(token_total, 1)).item())\n",
        "                train_sample_acc = float((sample_correct / max(sample_total, 1)).item())\n",
        "\n",
        "                steps_done = itr + 1\n",
        "                ll_iter = _safe_log10(steps_done)\n",
        "                ll_loss = _safe_log10(train_loss)\n",
        "                ll_lr = _safe_log10(lr)\n",
        "\n",
        "                ll_iters_hist.append(ll_iter)\n",
        "                ll_loss_hist.append(ll_loss)\n",
        "\n",
        "                slope = None\n",
        "                delta_ll_loss = None\n",
        "                plateau_now = 0\n",
        "\n",
        "                if (not plateau_reached) and (len(ll_iters_hist) >= cfg.plateau_window_logs) and (steps_done >= cfg.plateau_min_iters):\n",
        "                    xs = ll_iters_hist[-cfg.plateau_window_logs:]\n",
        "                    ys = ll_loss_hist[-cfg.plateau_window_logs:]\n",
        "                    slope = _linreg_slope(xs, ys)\n",
        "                    delta_ll_loss = ys[0] - ys[-1]\n",
        "                    is_improving = (delta_ll_loss >= 0.0)\n",
        "                    plateau_now = int(\n",
        "                        is_improving and (delta_ll_loss < cfg.plateau_log10_loss_delta) and (slope > -cfg.plateau_slope_threshold)\n",
        "                    )\n",
        "                    plateau_hits = (plateau_hits + 1) if plateau_now else 0\n",
        "                    if plateau_hits >= cfg.plateau_patience_logs:\n",
        "                        plateau_reached = True\n",
        "                        plateau_reached_iter = itr\n",
        "                        stop_after_log_iter = itr + cfg.log_interval * max(int(cfg.plateau_extra_logs), 0)\n",
        "                        print(\n",
        "                            f\"üü® plateau detected @ itr={itr} (slope={slope:.4f}, Œîlog10(loss)={delta_ll_loss:.4f}); \"\n",
        "                            f\"will stop after logging itr={stop_after_log_iter}\"\n",
        "                        )\n",
        "\n",
        "                now = time.time()\n",
        "                dt = now - last_log_t\n",
        "                last_log_t = now\n",
        "                print(\n",
        "                    f\"itr: {itr} lr: {lr:.3e} train loss: {train_loss:.6f} \"\n",
        "                    f\"train token_acc: {train_token_acc:.4f} train sample_acc: {train_sample_acc:.4f} (dt={dt:.2f}s)\"\n",
        "                )\n",
        "\n",
        "                log_dict = {\n",
        "                    \"iter\": steps_done,\n",
        "                    \"train/loss\": train_loss,\n",
        "                    \"train/token_acc\": train_token_acc,\n",
        "                    \"train/sample_acc\": train_sample_acc,\n",
        "                    \"lr\": lr,\n",
        "                    \"ll/iter\": ll_iter,\n",
        "                    \"ll/train_loss\": ll_loss,\n",
        "                    \"ll/lr\": ll_lr,\n",
        "                    \"plateau/now\": plateau_now,\n",
        "                    \"plateau/hits\": plateau_hits,\n",
        "                    \"plateau/reached\": int(plateau_reached),\n",
        "                    \"mix/bins_effective\": int(_mix_p_effective(cfg)),\n",
        "                    \"mix/enabled\": int(cfg.with_mixing),\n",
        "                }\n",
        "                if slope is not None:\n",
        "                    log_dict[\"plateau/loglog_slope\"] = float(slope)\n",
        "                if delta_ll_loss is not None:\n",
        "                    log_dict[\"plateau/delta_log10_loss_window\"] = float(delta_ll_loss)\n",
        "\n",
        "                # Test metrics (original eval distribution)\n",
        "                for k in test_block_sizes:\n",
        "                    test_loss, test_token_acc, test_sample_acc = eval_batch_metrics(\n",
        "                        model=model,\n",
        "                        device=device,\n",
        "                        cfg=cfg,\n",
        "                        block_size=int(k),\n",
        "                        use_amp=use_amp,\n",
        "                        amp_dtype=amp_dtype,\n",
        "                        batch_size=cfg.micro_batch_size,\n",
        "                    )\n",
        "                    last_test_metrics[int(k)] = {\"loss\": test_loss, \"token_acc\": test_token_acc, \"sample_acc\": test_sample_acc}\n",
        "                    log_dict[f\"test/K{int(k)}/loss\"] = test_loss\n",
        "                    log_dict[f\"test/K{int(k)}/token_acc\"] = test_token_acc\n",
        "                    log_dict[f\"test/K{int(k)}/sample_acc\"] = test_sample_acc\n",
        "\n",
        "                # NEW: sample accuracy by exact unique-count V on PRIMARY test K\n",
        "                if getattr(cfg, \"log_unique_count_metrics\", True):\n",
        "                    ubs = cfg.unique_count_eval_batch_size if cfg.unique_count_eval_batch_size is not None else cfg.micro_batch_size\n",
        "                    uniq_acc = eval_sample_acc_by_unique_count(\n",
        "                        model=model,\n",
        "                        device=device,\n",
        "                        cfg=cfg,\n",
        "                        block_size=int(primary_test_k),\n",
        "                        use_amp=use_amp,\n",
        "                        amp_dtype=amp_dtype,\n",
        "                        batch_size=int(ubs),\n",
        "                    )\n",
        "                    for V, sa in uniq_acc.items():\n",
        "                        log_dict[f\"test/K{int(primary_test_k)}/uniqueV{int(V)}/sample_acc\"] = float(sa)\n",
        "\n",
        "                # Threshold first-iter tracking using PRIMARY test K\n",
        "                pk_metrics = last_test_metrics.get(primary_test_k, None)\n",
        "                if pk_metrics is not None:\n",
        "                    pk_sa = float(pk_metrics[\"sample_acc\"])\n",
        "                    for thr in thresholds:\n",
        "                        if first_iter_at[thr] is None and pk_sa >= thr:\n",
        "                            first_iter_at[thr] = int(steps_done)\n",
        "\n",
        "                run.log(log_dict, step=steps_done)\n",
        "\n",
        "                if plateau_reached and (stop_after_log_iter is not None) and (itr >= stop_after_log_iter):\n",
        "                    iters_done = steps_done\n",
        "                    break\n",
        "\n",
        "            # ---- checkpointing ----\n",
        "            if cfg.ckpt_interval and (itr > 0) and (itr % cfg.ckpt_interval == 0):\n",
        "                steps_done = itr + 1\n",
        "                ckpt_name = make_save_filename(\"Checkpoint\", cfg, steps_done)\n",
        "                ckpt_path = os.path.join(cfg.save_dir, ckpt_name)\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"itr\": itr,\n",
        "                        \"iters_done\": steps_done,\n",
        "                        \"plateau_reached\": plateau_reached,\n",
        "                        \"plateau_reached_iter\": plateau_reached_iter,\n",
        "                        \"train_config\": wandb_cfg,\n",
        "                        \"model_config\": vars(model_cfg),\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                    },\n",
        "                    ckpt_path,\n",
        "                )\n",
        "                print(f\"saved checkpoint: {ckpt_path}\")\n",
        "                maybe_copy_to_drive(ckpt_path)\n",
        "\n",
        "    # Final eval snapshot\n",
        "    final_train_loss, final_train_token_acc, final_train_sample_acc = eval_batch_metrics(\n",
        "        model=model,\n",
        "        device=device,\n",
        "        cfg=cfg,\n",
        "        block_size=int(cfg.block_size),\n",
        "        use_amp=use_amp,\n",
        "        amp_dtype=amp_dtype,\n",
        "        batch_size=cfg.micro_batch_size,\n",
        "    )\n",
        "\n",
        "    final_test_snapshot = {}\n",
        "    for k in test_block_sizes:\n",
        "        tl, tta, tsa = eval_batch_metrics(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            cfg=cfg,\n",
        "            block_size=int(k),\n",
        "            use_amp=use_amp,\n",
        "            amp_dtype=amp_dtype,\n",
        "            batch_size=cfg.micro_batch_size,\n",
        "        )\n",
        "        final_test_snapshot[int(k)] = {\"loss\": tl, \"token_acc\": tta, \"sample_acc\": tsa}\n",
        "\n",
        "    # Final save\n",
        "    final_name = make_save_filename(\"Final\", cfg, iters_done)\n",
        "    final_path = os.path.join(cfg.save_dir, final_name)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"iters_done\": iters_done,\n",
        "            \"plateau_reached\": plateau_reached,\n",
        "            \"plateau_reached_iter\": plateau_reached_iter,\n",
        "            \"train_config\": wandb_cfg,\n",
        "            \"model_config\": vars(model_cfg),\n",
        "            \"model\": model.state_dict(),\n",
        "        },\n",
        "        final_path,\n",
        "    )\n",
        "    print(f\"saved final model: {final_path}\")\n",
        "    maybe_copy_to_drive(final_path)\n",
        "\n",
        "    run.summary[\"iters_done\"] = iters_done\n",
        "    run.summary[\"plateau_reached\"] = bool(plateau_reached)\n",
        "    run.summary[\"plateau_reached_iter\"] = int(plateau_reached_iter) if plateau_reached_iter is not None else None\n",
        "    run.finish()\n",
        "\n",
        "    # Build result dict for grid summary\n",
        "    res: Dict[str, Any] = dict(\n",
        "        vocab_n=int(cfg.vocab_n),\n",
        "        block_size=int(cfg.block_size),\n",
        "        n_layers=int(cfg.n_layers),\n",
        "        n_heads=int(cfg.n_heads),\n",
        "        without_pos=bool(cfg.without_pos),\n",
        "        use_mlp=bool(cfg.use_mlp),\n",
        "        allow_duplicates=bool(cfg.allow_duplicates),\n",
        "        with_mixing=bool(cfg.with_mixing),\n",
        "        mix_bins_effective=int(_mix_p_effective(cfg)),\n",
        "        vocab_over_embd=float(cfg.vocab_over_embd) if cfg.vocab_over_embd is not None else None,\n",
        "        n_embd_effective=int(eff_n_embd),\n",
        "        test_block_sizes=\",\".join(str(int(k)) for k in test_block_sizes),\n",
        "        primary_test_k=int(primary_test_k),\n",
        "        iters_done=int(iters_done),\n",
        "        plateau_reached=bool(plateau_reached),\n",
        "        plateau_reached_iter=int(plateau_reached_iter) if plateau_reached_iter is not None else None,\n",
        "        final_model_path=str(final_path),\n",
        "        wall_time_sec=float(time.time() - t0),\n",
        "\n",
        "        final_train_loss=float(final_train_loss),\n",
        "        final_train_token_acc=float(final_train_token_acc),\n",
        "        final_train_sample_acc=float(final_train_sample_acc),\n",
        "    )\n",
        "\n",
        "    for k, m in final_test_snapshot.items():\n",
        "        res[f\"final_test_loss_K{k}\"] = float(m[\"loss\"])\n",
        "        res[f\"final_test_token_acc_K{k}\"] = float(m[\"token_acc\"])\n",
        "        res[f\"final_test_sample_acc_K{k}\"] = float(m[\"sample_acc\"])\n",
        "\n",
        "    for thr in thresholds:\n",
        "        res[f\"iter_to_sample_acc_{thr}\"] = first_iter_at[thr]\n",
        "\n",
        "    return res\n",
        "\n",
        "# -------------------------\n",
        "# Grid runner (NEW: with_mixing is part of the grid)\n",
        "# -------------------------\n",
        "def run_grid(\n",
        "    base_cfg: TrainConfig,\n",
        "    vocab_sizes: Iterable[int],\n",
        "    layer_counts: Iterable[int],\n",
        "    block_sizes: Iterable[int],\n",
        "    without_pos_flags: Iterable[bool],\n",
        "    vocab_over_embd_list: Iterable[float],\n",
        "    allow_duplicates_flags: Optional[Iterable[bool]] = None,\n",
        "    use_mlp_flags: Optional[Iterable[bool]] = None,\n",
        "    with_mixing_flags: Optional[Iterable[bool]] = None,\n",
        "    print_summary: bool = True,\n",
        "):\n",
        "    vocab_sizes = sorted(list(vocab_sizes), reverse=True)\n",
        "    block_sizes = sorted(list(block_sizes), reverse=True)\n",
        "    layer_counts = sorted(list(layer_counts), reverse=True)\n",
        "    vocab_over_embd_list = sorted(list(vocab_over_embd_list))\n",
        "\n",
        "    if allow_duplicates_flags is None:\n",
        "        allow_duplicates_flags = [bool(base_cfg.allow_duplicates)]\n",
        "    else:\n",
        "        allow_duplicates_flags = list(allow_duplicates_flags)\n",
        "\n",
        "    if use_mlp_flags is None:\n",
        "        use_mlp_flags = [bool(base_cfg.use_mlp)]\n",
        "    else:\n",
        "        use_mlp_flags = list(use_mlp_flags)\n",
        "\n",
        "    if with_mixing_flags is None:\n",
        "        with_mixing_flags = [bool(base_cfg.with_mixing)]\n",
        "    else:\n",
        "        with_mixing_flags = list(with_mixing_flags)\n",
        "\n",
        "    # Default test block sizes = largest trained K in this grid\n",
        "    if base_cfg.test_block_sizes is None or len(base_cfg.test_block_sizes) == 0:\n",
        "        default_test_block_sizes = (max(int(k) for k in block_sizes),)\n",
        "    else:\n",
        "        default_test_block_sizes = tuple(int(k) for k in base_cfg.test_block_sizes)\n",
        "\n",
        "    results: list[Dict[str, Any]] = []\n",
        "\n",
        "    for N in vocab_sizes:\n",
        "        for K in block_sizes:\n",
        "            for L in layer_counts:\n",
        "                for npos in without_pos_flags:\n",
        "                    for mlp_on in use_mlp_flags:\n",
        "                        for dup in allow_duplicates_flags:\n",
        "                            for mix_on in with_mixing_flags:\n",
        "                                for r in vocab_over_embd_list:\n",
        "                                    cfg = copy.deepcopy(base_cfg)\n",
        "                                    cfg.vocab_n = int(N)\n",
        "                                    cfg.block_size = int(K)\n",
        "                                    cfg.n_layers = int(L)\n",
        "                                    cfg.without_pos = bool(npos)\n",
        "                                    cfg.use_mlp = bool(mlp_on)\n",
        "                                    cfg.allow_duplicates = bool(dup)\n",
        "                                    cfg.with_mixing = bool(mix_on)\n",
        "                                    cfg.vocab_over_embd = float(r)\n",
        "                                    cfg.test_block_sizes = default_test_block_sizes\n",
        "\n",
        "                                    try:\n",
        "                                        res = train_sorting_gpt(cfg)\n",
        "                                        res[\"error\"] = None\n",
        "                                    except Exception as e:\n",
        "                                        eff_n_embd = compute_effective_n_embd(cfg.vocab_n, cfg.n_heads, cfg.vocab_over_embd, cfg.n_embd)\n",
        "                                        res = {\n",
        "                                            \"vocab_n\": int(cfg.vocab_n),\n",
        "                                            \"block_size\": int(cfg.block_size),\n",
        "                                            \"n_layers\": int(cfg.n_layers),\n",
        "                                            \"n_heads\": int(cfg.n_heads),\n",
        "                                            \"without_pos\": bool(cfg.without_pos),\n",
        "                                            \"use_mlp\": bool(cfg.use_mlp),\n",
        "                                            \"allow_duplicates\": bool(cfg.allow_duplicates),\n",
        "                                            \"with_mixing\": bool(cfg.with_mixing),\n",
        "                                            \"mix_bins_effective\": int(_mix_p_effective(cfg)),\n",
        "                                            \"vocab_over_embd\": float(cfg.vocab_over_embd) if cfg.vocab_over_embd is not None else None,\n",
        "                                            \"n_embd_effective\": int(eff_n_embd),\n",
        "                                            \"test_block_sizes\": \",\".join(str(int(k)) for k in default_test_block_sizes),\n",
        "                                            \"primary_test_k\": int(max(default_test_block_sizes)),\n",
        "                                            \"iters_done\": None,\n",
        "                                            \"plateau_reached\": None,\n",
        "                                            \"plateau_reached_iter\": None,\n",
        "                                            \"final_model_path\": None,\n",
        "                                            \"final_train_loss\": None,\n",
        "                                            \"final_train_token_acc\": None,\n",
        "                                            \"final_train_sample_acc\": None,\n",
        "                                            \"error\": repr(e),\n",
        "                                        }\n",
        "                                        for thr in [0.9, 0.99, 0.999, 0.9999, 1.0]:\n",
        "                                            res[f\"iter_to_sample_acc_{thr}\"] = None\n",
        "\n",
        "                                    results.append(res)\n",
        "\n",
        "    if print_summary:\n",
        "        clear_console()\n",
        "        print(\"===== GRID SUMMARY =====\")\n",
        "        if pd is not None:\n",
        "            try:\n",
        "                from IPython.display import display\n",
        "            except Exception:\n",
        "                display = print\n",
        "\n",
        "            df = pd.DataFrame(results)\n",
        "\n",
        "            null_cols = [c for c in df.columns if c.startswith(\"iter_to_sample_acc_\")] + [\"iters_done\", \"plateau_reached_iter\"]\n",
        "            for c in null_cols:\n",
        "                if c in df.columns:\n",
        "                    df[c] = df[c].where(~df[c].isna(), \"NULL\")\n",
        "\n",
        "            pd.set_option(\"display.max_rows\", None)\n",
        "            pd.set_option(\"display.max_columns\", None)\n",
        "            pd.set_option(\"display.width\", 250)\n",
        "\n",
        "            sort_cols = [c for c in [\"vocab_n\", \"block_size\", \"n_layers\", \"without_pos\", \"use_mlp\", \"allow_duplicates\", \"with_mixing\", \"n_embd_effective\"] if c in df.columns]\n",
        "            if sort_cols:\n",
        "                df = df.sort_values(sort_cols).reset_index(drop=True)\n",
        "\n",
        "            display(df)\n",
        "            return df\n",
        "        else:\n",
        "            for row in results:\n",
        "                print(row)\n",
        "            return results\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "p3c1TPg8nmo9",
        "outputId": "14a0dabd-f2e6-4dbd-8e10-a83704178be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRID_ROOT = ./grid_outputs_grid_20260107_223822\n",
            "WANDB_DIR = ./grid_outputs_grid_20260107_223822/wandb\n",
            "SAVE_DIR  = ./grid_outputs_grid_20260107_223822/saved_models\n",
            "PROJECT   = sortgpt\n",
            "GROUP     = grid_20260107_223822\n",
            "Drive not mounted (or skipped). No Drive copying will occur.\n",
            "using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260107_223828-yccgogx1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nathan-henry-uc-berkeley-electrical-engineering-computer/sortgpt/runs/yccgogx1' target=\"_blank\">vocab128_blockSize16_layers3_pos0_mlp1_embd64_dup1_mix0</a></strong> to <a href='https://wandb.ai/nathan-henry-uc-berkeley-electrical-engineering-computer/sortgpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nathan-henry-uc-berkeley-electrical-engineering-computer/sortgpt' target=\"_blank\">https://wandb.ai/nathan-henry-uc-berkeley-electrical-engineering-computer/sortgpt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nathan-henry-uc-berkeley-electrical-engineering-computer/sortgpt/runs/yccgogx1' target=\"_blank\">https://wandb.ai/nathan-henry-uc-berkeley-electrical-engineering-computer/sortgpt/runs/yccgogx1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 0/60000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 0 lr: 4.975e-07 train loss: 4.848498 train token_acc: 0.0437 train sample_acc: 0.0000 (dt=1.22s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   0%|          | 264/60000 [00:05<14:51, 67.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 250 lr: 1.000e-04 train loss: 3.954088 train token_acc: 0.2191 train sample_acc: 0.0000 (dt=3.86s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   1%|          | 512/60000 [00:08<14:27, 68.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 500 lr: 9.999e-05 train loss: 2.686578 train token_acc: 0.8075 train sample_acc: 0.0320 (dt=3.65s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   1%|‚ñè         | 763/60000 [00:12<14:28, 68.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 750 lr: 9.998e-05 train loss: 1.670355 train token_acc: 0.9392 train sample_acc: 0.4102 (dt=3.65s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   2%|‚ñè         | 1013/60000 [00:16<14:24, 68.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 1000 lr: 9.996e-05 train loss: 0.940036 train token_acc: 0.9726 train sample_acc: 0.6702 (dt=3.65s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   2%|‚ñè         | 1263/60000 [00:19<14:20, 68.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 1250 lr: 9.992e-05 train loss: 0.500542 train token_acc: 0.9841 train sample_acc: 0.7917 (dt=3.65s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   3%|‚ñé         | 1513/60000 [00:23<14:16, 68.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 1500 lr: 9.988e-05 train loss: 0.288355 train token_acc: 0.9884 train sample_acc: 0.8469 (dt=3.65s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   3%|‚ñé         | 1741/60000 [00:26<14:56, 65.00it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1022346776.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m df_summary = run_grid(\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mbase_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mvocab_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1692680914.py\u001b[0m in \u001b[0;36mrun_grid\u001b[0;34m(base_cfg, vocab_sizes, layer_counts, block_sizes, without_pos_flags, vocab_over_embd_list, allow_duplicates_flags, use_mlp_flags, with_mixing_flags, print_summary)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m                                         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sorting_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m                                         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1692680914.py\u001b[0m in \u001b[0;36mtrain_sorting_gpt\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_to_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                     \u001b[0mloss_to_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# choose config + run grid\n",
        "# =========================\n",
        "import os, time\n",
        "\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "PROJECT = globals().get(\"PROJECT\", \"sortgpt\")\n",
        "GROUP = f\"grid_{STAMP}\"\n",
        "\n",
        "GRID_ROOT = f\"./grid_outputs_{GROUP}\"\n",
        "WANDB_DIR = os.path.join(GRID_ROOT, \"wandb\")\n",
        "SAVE_DIR  = os.path.join(GRID_ROOT, \"saved_models\")\n",
        "\n",
        "os.makedirs(WANDB_DIR, exist_ok=True)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Put all wandb local artifacts for the whole grid under one directory\n",
        "os.environ[\"WANDB_DIR\"] = WANDB_DIR\n",
        "os.environ[\"WANDB_CACHE_DIR\"] = os.path.join(GRID_ROOT, \"wandb_cache\")\n",
        "os.environ[\"WANDB_CONFIG_DIR\"] = os.path.join(GRID_ROOT, \"wandb_config\")\n",
        "\n",
        "print(\"GRID_ROOT =\", GRID_ROOT)\n",
        "print(\"WANDB_DIR =\", WANDB_DIR)\n",
        "print(\"SAVE_DIR  =\", SAVE_DIR)\n",
        "print(\"PROJECT   =\", PROJECT)\n",
        "print(\"GROUP     =\", GROUP)\n",
        "\n",
        "# OPTIONAL: if Drive is mounted, copy into a per-grid folder\n",
        "if globals().get(\"GDRIVE_MOUNTED\", False) and globals().get(\"GDRIVE_BASE_DIR\", None):\n",
        "    GDRIVE_MODEL_DIR = os.path.join(GDRIVE_BASE_DIR, GROUP)\n",
        "    os.makedirs(GDRIVE_MODEL_DIR, exist_ok=True)\n",
        "    print(\"‚úÖ Drive destination set for this grid:\")\n",
        "    print(\"   GDRIVE_MODEL_DIR =\", GDRIVE_MODEL_DIR)\n",
        "else:\n",
        "    print(\"Drive not mounted (or skipped). No Drive copying will occur.\")\n",
        "\n",
        "base_cfg = TrainConfig(\n",
        "    vocab_n=256,              # overridden by grid\n",
        "    block_size=16,            # overridden by grid\n",
        "\n",
        "    allow_duplicates=True,    # MUST BE TRUE if using mixing flags\n",
        "    with_mixing=False,        # overridden by grid axis below\n",
        "    mixing_bins=8,            # when with_mixing=True -> effective p=8\n",
        "\n",
        "    n_layers=2,               # overridden by grid\n",
        "    n_heads=1,\n",
        "\n",
        "    n_embd=64,                # used only if vocab_over_embd=None (grid sets vocab_over_embd)\n",
        "    vocab_over_embd=None,\n",
        "\n",
        "    without_pos=False,        # overridden by grid\n",
        "    use_mlp=True,             # overridden by grid\n",
        "\n",
        "    test_block_sizes=None,    # default => largest K in the grid\n",
        "\n",
        "    max_iters=60000,\n",
        "    warmup_iters=200,\n",
        "    learning_rate=1e-4,\n",
        "    min_lr=1e-6,\n",
        "    weight_decay=0.0,\n",
        "\n",
        "    micro_batch_size=4096,\n",
        "    effective_batch_size=4096,\n",
        "\n",
        "    log_interval=250,\n",
        "    ckpt_interval=1000000,    # huge => usually no checkpoints; only finals\n",
        "    save_dir=SAVE_DIR,\n",
        "\n",
        "    plateau_window_logs=40,\n",
        "    plateau_min_iters=20000,\n",
        "    plateau_slope_threshold=0.02,\n",
        "    plateau_log10_loss_delta=0.02,\n",
        "    plateau_patience_logs=2,\n",
        "    plateau_extra_logs=1,\n",
        "\n",
        "    # Unique-count metrics\n",
        "    log_unique_count_metrics=True,\n",
        "    unique_count_eval_batch_size=None,   # set smaller (e.g. 2048) if you want faster logs\n",
        "\n",
        "    wandb_project=PROJECT,\n",
        "    wandb_group=GROUP,\n",
        ")\n",
        "\n",
        "df_summary = run_grid(\n",
        "    base_cfg,\n",
        "    vocab_sizes=[128],\n",
        "    layer_counts=[1, 2, 3],\n",
        "    block_sizes=[16],\n",
        "    without_pos_flags=[False],\n",
        "    vocab_over_embd_list=[2, 4, 8],\n",
        "\n",
        "    use_mlp_flags=[True, False],\n",
        "\n",
        "    # mix0 => with_mixing=False => effective p=1\n",
        "    # mix1 => with_mixing=True  => effective p=8\n",
        "    with_mixing_flags=[False, True], # ONLY SET TO TRUE IF YOU ARE ALLOWING DUPLICATES !!!!!!!!!!!!!!!!!!\n",
        "\n",
        "    print_summary=True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
