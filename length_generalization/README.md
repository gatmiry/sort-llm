# Length Generalization Experiments for Sorting Task

This directory contains code, checkpoints, and results for systematic length generalization experiments on small transformer models trained to sort sequences of numbers.

## Overview

We trained small GPT-style transformers on sorting tasks and systematically evaluated their ability to generalize from short training sequences to much longer test sequences (K=17-32). The experiments explore how different architectural choices, data loading strategies, and training regimes affect length generalization.

## Directory Structure

```
length_generalization/
‚îú‚îÄ‚îÄ code/                          # Training and analysis scripts
‚îÇ   ‚îú‚îÄ‚îÄ sortGPT_len_generalization.py       # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ plot_len_generalization_results.py  # Plotting script (basic)
‚îÇ   ‚îú‚îÄ‚îÄ plot_comprehensive_results.py       # Comprehensive analysis and plotting
‚îÇ   ‚îú‚îÄ‚îÄ run_len_generalization.sh          # SLURM script: dynamic training (MLP off)
‚îÇ   ‚îú‚îÄ‚îÄ run_len_generalization_single_k.sh # SLURM script: fixed single-K training
‚îÇ   ‚îú‚îÄ‚îÄ run_len_generalization_no_ln.sh    # SLURM script: no layer norm
‚îÇ   ‚îî‚îÄ‚îÄ run_len_generalization_single_k_no_ln.sh  # SLURM script: single-K, no layer norm
‚îú‚îÄ‚îÄ checkpoints/                   # Model checkpoints (organized by experiment)
‚îÇ   ‚îú‚îÄ‚îÄ with_layernorm/           # Standard runs with LayerNorm (~776MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic/              # Multi-length training (len_gen_k4...k16)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fixed/                # Single-K training (len_gen_single_k4...k16)
‚îÇ   ‚îú‚îÄ‚îÄ without_layernorm/        # Runs without LayerNorm (in progress)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic/              # Multi-length, no layernorm
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fixed/                # Single-K, no layernorm
‚îÇ   ‚îú‚îÄ‚îÄ README.md                 # Checkpoint organization guide
‚îÇ   ‚îî‚îÄ‚îÄ TASK_MAPPING.md           # Task ID ‚Üí configuration mapping
‚îú‚îÄ‚îÄ results/                       # Plots, CSVs, and analysis outputs
‚îÇ   ‚îú‚îÄ‚îÄ len_gen_comprehensive.png
‚îÇ   ‚îú‚îÄ‚îÄ len_gen_mlp_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ len_gen_fixed_vs_dynamic_full.png
‚îÇ   ‚îú‚îÄ‚îÄ len_gen_comprehensive_summary.csv
‚îÇ   ‚îî‚îÄ‚îÄ len_gen_findings_report.txt
‚îî‚îÄ‚îÄ README.md                      # This file
```

**Note**: Checkpoint `.pt` files are excluded from git by default (see `.gitignore`). The directory structure and metadata are included, but the actual model weights need to be synced separately or added with `git lfs`.

## Experimental Setup

### Task
- **Input**: Sequence of N random numbers from vocabulary [0, 127]
- **Output**: Sorted sequence (ascending order)
- **Format**: `[num1, num2, ..., numN] SEP [sorted1, sorted2, ..., sortedN]`

### Model Architecture
- **Base**: GPT-style decoder-only transformer
- **Vocabulary**: 128 numbers + 1 separator token = 129 total
- **Embedding dim**: 128
- **Heads**: 1 (single-head attention)
- **Layers**: 1 or 2
- **No positional embeddings** (all experiments)

### Variables Explored

#### 1. **Model Configuration**
- **Layers**: 1 vs 2
- **MLP**: On (full transformer block) vs Off (attention-only)
- **LayerNorm**: On (standard) vs Off (no normalization)

#### 2. **Data Loading Strategy**
- **Mix**: Each batch samples K uniformly from [train_min_k, train_max_k]
- **Curriculum**: K sampled from [train_min_k, max_k(t)] where max_k grows from min‚Üímax over training

#### 3. **Training Regime**
- **Dynamic**: Train on range of lengths (e.g., K=2‚Üí8)
- **Fixed**: Train on single length only (e.g., K=4)

#### 4. **Training Length Range**
- **train_max_k**: 4, 6, 8, 10, 12, 14, 16
- **train_min_k**: 2 (for dynamic), same as max (for fixed)

### Evaluation
- **Test lengths**: K=17-32 (always out-of-distribution)
- **Metrics**: 
  - Exact match accuracy (entire sequence correct)
  - Token accuracy (per-position correctness)
- **Samples**: 100 per length

### Training Details
- **Iterations**: 40,000
- **Warmup**: 400 iterations
- **Learning rate**: 1e-4 ‚Üí 1e-6 (cosine decay)
- **Batch size**: 4096 samples
- **Seeds**: 3 independent runs per configuration

## Key Findings

### 1. Simplest Model Generalizes Best from Minimal Data
**Result**: 1-layer attention-only (no MLP) trained on just K=4 achieves **98% exact-match accuracy on K=17-32**.

This **outperforms** dynamic training at the same max K:
- Fixed K=4: 97.9% ¬± 2.4%
- Dynamic K2‚Üí4 (mix): 90.2% ¬± 3.6%
- Dynamic K2‚Üí4 (curriculum): 67.9% ¬± 1.3%

**Implication**: Pure attention learns a surprisingly general sorting algorithm from minimal examples. Data diversity doesn't always help‚Äîfocused training on a single (harder) length can be more effective for simple models.

### 2. MLP and Depth Hurt Generalization at Small K
At training max K=4:
- **L1, no MLP**: 97.9% generalization ‚úì
- **L1, with MLP**: 62.2%
- **L2, no MLP**: 1.9% ‚úó
- **L2, with MLP**: 0.7% ‚úó

Deeper/wider models need more diverse training data (K‚â•10-12) to reach 95%+ generalization.

### 3. Mix > Curriculum at Small K
Dynamic **mix** consistently converges faster than **curriculum** at small training lengths:
- K=6: mix 96.3% vs curriculum 87.2%
- K=8: mix 99.7% vs curriculum 97.5%

Curriculum catches up at larger K (‚â•12).

### 4. Dynamic vs Fixed Training
For **L1 no-MLP**: Fixed training is **better** (simpler is better)
For **L2 with MLP** at K=8: Dynamic much better (96% vs 64%)

The optimal training regime depends on model capacity.

### 5. Minimum K for ‚â•95% Generalization

| Configuration | Min train_max_k |
|--------------|----------------|
| L1, no MLP, Fixed | **K=4** |
| L1, no MLP, Dynamic Mix | K=6 |
| L1, with MLP, Dynamic Mix | K=6 |
| L1, with MLP, Dynamic Curriculum | K=8 |
| L2, with MLP, Dynamic Mix | K=8 |
| L2, no MLP, Dynamic Mix | K=10 |
| L2, with MLP, Dynamic Curriculum | **K=12** |

## Experiments Completed

### With LayerNorm (~328 runs)
- ‚úì 1-2 layers √ó MLP on/off √ó mix/curriculum √ó K=4,6,8,10,12,14,16 √ó 3 seeds
- ‚úì 1-2 layers √ó MLP on/off √ó fixed K=4,6,8,10,12,14,16 √ó 3 seeds

### Without LayerNorm (168 runs, in progress)
- üîÑ 1-2 layers √ó MLP on/off √ó mix/curriculum √ó K=4,6,8,10,12,14,16 √ó 3 seeds (84 jobs)
- üîÑ 1-2 layers √ó MLP on/off √ó fixed K=4,6,8,10,12,14,16 √ó 3 seeds (84 jobs)

## How to Use

### Run Training
```bash
# Dynamic training (multi-length) without MLP
sbatch code/run_len_generalization.sh

# Fixed single-K training
sbatch code/run_len_generalization_single_k.sh

# Without layer norm variants
sbatch code/run_len_generalization_no_ln.sh
sbatch code/run_len_generalization_single_k_no_ln.sh
```

### Generate Analysis
```bash
# Comprehensive analysis with all plots and summaries
python code/plot_comprehensive_results.py

# Basic plots (older, simpler version)
python code/plot_len_generalization_results.py
```

### Understanding Task Directories

Each experiment group (e.g., `len_gen_k8`) contains **4 task directories** organized by (layers, length_mode or MLP):

**For dynamic training** (e.g., `len_gen_k8/`):
- `task_0000/`: 2 layers, mix ‚Äî contains **both MLP on and MLP off** checkpoints, 3 seeds each
- `task_0001/`: 2 layers, curriculum ‚Äî contains **both MLP on and MLP off**, 3 seeds each
- `task_0002/`: 1 layer, mix ‚Äî contains **both MLP on and MLP off**, 3 seeds each (‚≠ê MLP off = best)
- `task_0003/`: 1 layer, curriculum ‚Äî contains **both MLP on and MLP off**, 3 seeds each

**For fixed single-K training** (e.g., `len_gen_single_k4/`):
- `task_0000/`: 2 layers, MLP on (3 seeds)
- `task_0001/`: 2 layers, MLP off (3 seeds)
- `task_0002/`: 1 layer, MLP on (3 seeds)
- `task_0003/`: 1 layer, MLP off (3 seeds) (‚≠ê ~98% generalization at K=4!)

‚≠ê To find specific checkpoints, **look at the filename**: `_mlp0_` = no MLP, `_mlp1_` = with MLP

See `checkpoints/TASK_MAPPING.md` for complete details and how to decode task directories.

### Checkpoint Organization
See `checkpoints/README.md` for details on checkpoint naming and organization.

Checkpoints are named:
```
{Prefix}_N128_d128_H1_L{layers}_npos1_mlp{0|1}_ln{0|1}_len{mix|curriculum}_trainK{min}to{max}_testK17to32_nodup1_iters{step}.pt
```

Example:
```
Final_N128_d128_H1_L1_npos1_mlp0_ln1_lenmix_trainK4to4_testK17to32_nodup1_iters40000.pt
```
- L1 = 1 layer
- mlp0 = no MLP (attention-only)
- ln1 = with LayerNorm
- lenmix = uniform mixing data loading
- trainK4to4 = fixed training on K=4 only

## Citation & Related Work

This work builds on insights from:
- "Grokking" phenomena in algorithmic tasks
- Length generalization in transformers
- Curriculum learning and data ordering

The repository is based on [gatmiry/sort-llm](https://github.com/gatmiry/sort-llm).

## Contact

For questions about these experiments:
- Shan Chen (shan.chen@childrens.harvard.edu)
