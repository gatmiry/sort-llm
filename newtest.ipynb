{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba2904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb676ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m checkpoint_path = \u001b[33m'\u001b[39m\u001b[33msaved_models/jan27-tbyt_without-pos-embedding_n_embd:64_head:1_layers:2_vocab_size:128_itr:60000_checkpoint_time_1769551758.5343654.pt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#'Grid_training_without_duplicates/Final_N64_K16_L2_H1_E16_r4over1_npos1_mlp1_dup0_testK16_iters60000.pt'\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#'Grid_training_without_duplicates/Final_N128_K16_L2_H1_E64_r2over1_npos0_mlp1_dup0_testK16_iters60000.pt'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model, checkpoint = \u001b[43mload_checkpoint_to_gpt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGPTConfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/sorting/sort-llm/checkpoint_translator.py:129\u001b[39m, in \u001b[36mload_checkpoint_to_gpt\u001b[39m\u001b[34m(checkpoint_path, GPT, GPTConfig, device, strict, config, model_type)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[32m    127\u001b[39m checkpoint = torch.load(checkpoint_path, map_location=device)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m model_config = \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_config\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    130\u001b[39m saved_state_dict = checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Check that this is an MLP model\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'model_config'"
     ]
    }
   ],
   "source": [
    "from model_tbyt_3 import GPT, GPTConfig\n",
    "from checkpoint_translator import load_checkpoint_to_gpt\n",
    "checkpoint_path = 'Grid_training_without_duplicates/Final_N64_K16_L2_H1_E16_r4over1_npos1_mlp1_dup0_testK16_iters60000.pt'\n",
    "#'Grid_training_without_duplicates/Final_N128_K16_L2_H1_E64_r2over1_npos0_mlp1_dup0_testK16_iters60000.pt'\n",
    "model, checkpoint = load_checkpoint_to_gpt(\n",
    "    checkpoint_path, \n",
    "    GPT, \n",
    "    GPTConfig,\n",
    "    device='cpu',\n",
    "    strict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0094a340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(129, 64)\n",
       "    (wpe): Embedding(129, 64)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (c_attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (c_fc): MLP(\n",
       "          (fc_1): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (fc_2): Linear(in_features=192, out_features=64, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_tbyt_3 import GPT, GPTConfig\n",
    "import torch\n",
    "import os\n",
    "itr_num = 60000\n",
    "block_size = 32\n",
    "vocab_size = 128\n",
    "device = 'cpu'\n",
    "config = GPTConfig(block_size=block_size, vocab_size=vocab_size)\n",
    "config.without_pos = True\n",
    "model = GPT(config)\n",
    "model_state_dict = torch.load(os.path.join(os.getcwd(), f'saved_models/jan27-tbyt_without-pos-embedding_n_embd:64_head:1_layers:2_vocab_size:128_itr:60000_checkpoint_time_1769551758.5343654.pt'), map_location=device)['model']\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4600074",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# Use the loaded model's config instead of class defaults\n",
    "vocab_size = model.config.vocab_size\n",
    "block_size = model.config.block_size\n",
    "#test_1_seq = torch.cat((torch.arange(110, 30, -5) , torch.arange(127, 111, -1)), dim=0) \n",
    "#print(test_1_seq)\n",
    "def get_batch(changing_num=-1, changing_index=-1, initial_sequence=None, batch_size=batch_size):\n",
    "   def cat_sorted_tensor(x):\n",
    "      if initial_sequence is not None:\n",
    "         x = initial_sequence\n",
    "      else:\n",
    "         x = x\n",
    "         #x, _ = torch.sort(x, descending=True)\n",
    "      if changing_num != -1:\n",
    "         if changing_index == -1:\n",
    "            x[0] = changing_num\n",
    "         else:\n",
    "            x[changing_index] = changing_num\n",
    "      #x = torch.cat((torch.tensor([100]).repeat(16), torch.tensor([1]).repeat(16)))\n",
    "      #x = torch.tensor([100,100,100,100,1,1,1,1])\n",
    "      vals, _ = torch.sort(x)\n",
    "      #vals2, _ = torch.sort(x, descending=True)\n",
    "      #print('vals are ', vals)\n",
    "      return torch.cat((x, torch.tensor([vocab_size]), vals), dim=0)\n",
    "   #x = torch.stack([cat_sorted_tensor(torch.randperm(vocab_size)[:block_size]) for _ in range(batch_size)])\n",
    "   x = torch.stack([cat_sorted_tensor(torch.randperm(vocab_size)[:block_size]) for _ in range(batch_size)])\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601569bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx dim is  torch.Size([1, 65])\n",
      "layer_n is  0\n",
      "layer_n is  0\n",
      "layer_n is  1\n",
      "layer_n is  1\n",
      "loss is  0.0006695028860121965\n",
      "idx is: tensor([[  2,  76, 120,  68, 107,  95,  17,  47,  73, 123,  38,   6, 116,  20,\n",
      "           5,  12,  13,  67,  99,  39, 102,  58,  45,  97,  87,  85,  29, 106,\n",
      "          61,  21,  26,  42, 128,   2,   5,   6,  12,  13,  17,  20,  21,  26,\n",
      "          29,  38,  39,  42,  45,  47,  58,  61,  67,  68,  73,  76,  85,  87,\n",
      "          95,  97,  99, 102, 106, 107, 116, 120, 123]])\n",
      "model output is  tensor([[ 22,  97, 120,  76, 120, 107,  35,  66,  76, 123,  47,  17, 120,  38,\n",
      "           6,  17,  17,  68, 107,  47, 107,  67,  47,  99,  95,  87,  38, 107,\n",
      "          67,  29,  29,  45,   2,   5,   6,  12,  13,  17,  20,  21,  26,  29,\n",
      "          38,  39,  42,  45,  47,  58,  61,  67,  68,  73,  76,  85,  87,  95,\n",
      "          97,  99, 102, 106, 107, 116, 120, 123, 123]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "idx = get_batch()\n",
    "print('idx dim is ', idx.shape)\n",
    "logits, loss = model(idx)\n",
    "print('loss is ', loss.item())\n",
    "print(f'idx is: {idx}')\n",
    "print('model output is ', torch.argmax(logits, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b893f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
