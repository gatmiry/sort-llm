{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NlbPLmYwHhe",
        "outputId": "a293df50-68e2-4922-ed3d-46b38d5f4c15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ./wandb_grid_bs8192_20260106_221332/wandb/ wasn't writable, using system temp directory\n"
          ]
        }
      ],
      "source": [
        "!pip -q install wandb\n",
        "\n",
        "import os, math, time, contextlib, copy\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import amp\n",
        "from tqdm import trange\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# ---- Speed knobs ----\n",
        "def enable_tf32():\n",
        "    if torch.cuda.is_available():\n",
        "        if hasattr(torch.backends.cuda.matmul, \"fp32_precision\"):\n",
        "            torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
        "        else:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "        if (\n",
        "            hasattr(torch.backends, \"cudnn\")\n",
        "            and hasattr(torch.backends.cudnn, \"conv\")\n",
        "            and hasattr(torch.backends.cudnn.conv, \"fp32_precision\")\n",
        "        ):\n",
        "            torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
        "        else:\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "enable_tf32()\n",
        "\n",
        "def flash_sdpa_context():\n",
        "    \"\"\"Request FlashAttention kernels when available; safe fallback otherwise.\"\"\"\n",
        "    try:\n",
        "        from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "        return sdpa_kernel([SDPBackend.FLASH_ATTENTION])\n",
        "    except Exception:\n",
        "        return contextlib.nullcontext()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8JvsirmnfRv"
      },
      "outputs": [],
      "source": [
        "import os, time, copy, math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Tuple, Iterable\n",
        "from fractions import Fraction\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "import wandb\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Robust context helpers (no device_type kwarg anywhere)\n",
        "# -------------------------\n",
        "def get_sdpa_context():\n",
        "    \"\"\"\n",
        "    Returns a real context-manager object (not a generator).\n",
        "    Uses torch.nn.attention.sdpa_kernel if available; otherwise no-op.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "        return sdpa_kernel([SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH])\n",
        "    except Exception:\n",
        "        return nullcontext()\n",
        "\n",
        "def get_autocast_context(device: torch.device, dtype: Optional[torch.dtype]):\n",
        "    \"\"\"\n",
        "    Returns a real context-manager object for autocast.\n",
        "    Never passes device_type=... (compat across torch versions).\n",
        "    \"\"\"\n",
        "    if device.type != \"cuda\" or dtype is None:\n",
        "        return nullcontext()\n",
        "\n",
        "    # Prefer torch.amp.autocast if present; else fallback to torch.cuda.amp.autocast\n",
        "    try:\n",
        "        return torch.amp.autocast(\"cuda\", dtype=dtype)          # positional device arg\n",
        "    except Exception:\n",
        "        return torch.cuda.amp.autocast(dtype=dtype)             # no device_type kwarg\n",
        "\n",
        "def make_grad_scaler(enabled: bool):\n",
        "    \"\"\"\n",
        "    Returns a scaler-like object with .is_enabled(), .scale(), .step(), .update().\n",
        "    \"\"\"\n",
        "    if not enabled:\n",
        "        class _NoScaler:\n",
        "            def is_enabled(self): return False\n",
        "            def scale(self, x): return x\n",
        "            def step(self, opt): opt.step()\n",
        "            def update(self): pass\n",
        "        return _NoScaler()\n",
        "\n",
        "    # Prefer torch.amp.GradScaler if present; else torch.cuda.amp.GradScaler\n",
        "    try:\n",
        "        return torch.amp.GradScaler()\n",
        "    except Exception:\n",
        "        return torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Model components\n",
        "# -------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.fc_2 = nn.Linear(3 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc_2(self.gelu(self.fc_1(x)))\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_heads == 0\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.n_embd // config.n_heads\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.use_mlp = bool(getattr(config, \"use_mlp\", True))\n",
        "\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        if self.use_mlp:\n",
        "            self.mlp = MLP(config)\n",
        "            self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        else:\n",
        "            self.mlp = None\n",
        "            self.ln_2 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        if self.mlp is not None:\n",
        "            x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_size: int,\n",
        "        vocab_size: int,\n",
        "        n_layers: int = 2,\n",
        "        n_heads: int = 1,\n",
        "        n_embd: int = 64,\n",
        "        without_pos: bool = False,\n",
        "        use_mlp: bool = True,                 # NEW\n",
        "        max_seq_len: Optional[int] = None,\n",
        "    ):\n",
        "        self.block_size = int(block_size)\n",
        "        self.vocab_size = int(vocab_size)\n",
        "        self.n_layers = int(n_layers)\n",
        "        self.n_heads = int(n_heads)\n",
        "        self.n_embd = int(n_embd)\n",
        "        self.without_pos = bool(without_pos)\n",
        "        self.use_mlp = bool(use_mlp)           # NEW\n",
        "        self.max_seq_len = int(max_seq_len if max_seq_len is not None else (2 * self.block_size + 1))\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_layers = config.n_layers\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.max_seq_len, config.n_embd),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.transformer.wte.weight  # weight tying\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.register_buffer(\"pos_idx\", torch.arange(config.max_seq_len), persistent=False)\n",
        "\n",
        "        # Positional encodings fixed to 0 (and frozen) if without_pos=True\n",
        "        if self.config.without_pos:\n",
        "            with torch.no_grad():\n",
        "                self.transformer.wpe.weight.zero_()\n",
        "            self.transformer.wpe.weight.requires_grad_(False)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
        "                std *= (2 * self.n_layers) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx, return_full_logits: bool = False, block_size: Optional[int] = None):\n",
        "        B, T = idx.size()\n",
        "        if block_size is None:\n",
        "            block_size = self.config.block_size\n",
        "        block_size = int(block_size)\n",
        "\n",
        "        expected_T = 2 * block_size + 1\n",
        "        assert T == expected_T, f\"Expected T={expected_T} for block_size={block_size}, got T={T}\"\n",
        "        assert T <= self.config.max_seq_len, f\"T={T} exceeds max_seq_len={self.config.max_seq_len}\"\n",
        "\n",
        "        pos = self.transformer.wpe(self.pos_idx[:T])\n",
        "        x = self.transformer.wte(idx) if self.config.without_pos else (self.transformer.wte(idx) + pos)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        targets = idx[:, block_size + 1 :]  # (B, K)\n",
        "\n",
        "        if return_full_logits:\n",
        "            logits = self.lm_head(x)                       # (B, T, V)\n",
        "            logits_for_loss = logits[:, block_size:T-1, :] # (B, K, V)\n",
        "        else:\n",
        "            x_for_loss = x[:, block_size:T-1, :]           # (B, K, C)\n",
        "            logits_for_loss = self.lm_head(x_for_loss)     # (B, K, V)\n",
        "            logits = logits_for_loss\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits_for_loss.reshape(-1, logits_for_loss.size(-1)),\n",
        "            targets.reshape(-1),\n",
        "        )\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Data batching\n",
        "# -------------------------\n",
        "SEP_TOKEN = \"SEP\"\n",
        "\n",
        "def _sample_numbers(batch_size: int, vocab_n: int, block_size: int, device: torch.device, allow_duplicates: bool):\n",
        "    if allow_duplicates:\n",
        "        return torch.randint(0, vocab_n, (batch_size, block_size), device=device, dtype=torch.long)\n",
        "\n",
        "    if block_size > vocab_n:\n",
        "        raise ValueError(f\"allow_duplicates=False requires block_size <= vocab_n (got block_size={block_size}, vocab_n={vocab_n})\")\n",
        "\n",
        "    scores = torch.rand(batch_size, vocab_n, device=device)\n",
        "    return scores.topk(block_size, dim=1).indices.to(torch.long)\n",
        "\n",
        "def get_batch(batch_size: int, device: torch.device, vocab_n: int, block_size: int, allow_duplicates: bool):\n",
        "    x = _sample_numbers(batch_size, vocab_n, block_size, device, allow_duplicates)\n",
        "    vals = x.sort(dim=1).values\n",
        "    sep_id = vocab_n\n",
        "    sep = torch.full((batch_size, 1), sep_id, device=device, dtype=torch.long)\n",
        "    return torch.cat([x, sep, vals], dim=1)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# LR + plateau helpers\n",
        "# -------------------------\n",
        "def create_optimizer(model, weight_decay: float, lr: float):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    decay_params = [p for p in params if p.dim() > 1]\n",
        "    nondecay_params = [p for p in params if p.dim() <= 1]\n",
        "    optim_groups = [\n",
        "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "        {\"params\": nondecay_params, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    try:\n",
        "        return torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=True)\n",
        "    except TypeError:\n",
        "        return torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "def get_lr(itr: int, cfg) -> float:\n",
        "    if itr < cfg.warmup_iters:\n",
        "        return cfg.learning_rate * (itr + 1) / (cfg.warmup_iters + 1)\n",
        "    if itr > cfg.max_iters:\n",
        "        return cfg.min_lr\n",
        "    ratio = (itr - cfg.warmup_iters) / (cfg.max_iters - cfg.warmup_iters)\n",
        "    ratio = 0.5 * (1.0 + math.cos(math.pi * ratio))\n",
        "    return cfg.min_lr + ratio * (cfg.learning_rate - cfg.min_lr)\n",
        "\n",
        "def _safe_log10(x: float, eps: float = 1e-30) -> float:\n",
        "    return math.log10(max(float(x), eps))\n",
        "\n",
        "def _linreg_slope(xs, ys) -> float:\n",
        "    n = len(xs)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "    x_mean = sum(xs) / n\n",
        "    y_mean = sum(ys) / n\n",
        "    cov = 0.0\n",
        "    var = 0.0\n",
        "    for x, y in zip(xs, ys):\n",
        "        dx = x - x_mean\n",
        "        dy = y - y_mean\n",
        "        cov += dx * dy\n",
        "        var += dx * dx\n",
        "    return cov / var if var > 0 else 0.0\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Ratio + embedding dim helpers\n",
        "# -------------------------\n",
        "def _ratio_to_tag(r: Optional[float]) -> str:\n",
        "    if r is None:\n",
        "        return \"none\"\n",
        "    fr = Fraction(float(r)).limit_denominator(1024)\n",
        "    return f\"{fr.numerator}over{fr.denominator}\"\n",
        "\n",
        "def _block_sizes_to_tag(ks: Tuple[int, ...]) -> str:\n",
        "    return \"K\" + \"-\".join(str(int(k)) for k in ks)\n",
        "\n",
        "def compute_effective_n_embd(vocab_n: int, n_heads: int, vocab_over_embd: Optional[float], fallback_n_embd: int) -> int:\n",
        "    \"\"\"\n",
        "    Implements: (vocab_size / embd_dim) = r  => embd_dim = vocab_size / r\n",
        "    vocab_size includes SEP => total_vocab = vocab_n + 1\n",
        "    \"\"\"\n",
        "    if vocab_over_embd is None:\n",
        "        n_embd = int(fallback_n_embd)\n",
        "    else:\n",
        "        r = float(vocab_over_embd)\n",
        "        if r <= 0:\n",
        "            raise ValueError(f\"vocab_over_embd must be > 0, got {vocab_over_embd}\")\n",
        "        total_vocab_size = int(vocab_n) + 1\n",
        "        n_embd = int(round(total_vocab_size / r))\n",
        "        n_embd = max(1, n_embd)\n",
        "\n",
        "    n_heads = int(n_heads)\n",
        "    if n_embd % n_heads != 0:\n",
        "        n_embd = ((n_embd + n_heads - 1) // n_heads) * n_heads\n",
        "    return int(n_embd)\n",
        "\n",
        "def acc_from_logits(logits: torch.Tensor, idx: torch.Tensor, block_size: int):\n",
        "    targets = idx[:, block_size + 1 :]  # (B, K)\n",
        "    preds = logits.argmax(dim=-1)       # (B, K)\n",
        "    token_acc = (preds == targets).float().mean()\n",
        "    sample_acc = (preds == targets).all(dim=1).float().mean()\n",
        "    return token_acc, sample_acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_batch_metrics(model: nn.Module, device: torch.device, cfg, block_size: int, use_amp: bool, amp_dtype: Optional[torch.dtype], batch_size: Optional[int] = None):\n",
        "    model.eval()\n",
        "    bs = int(batch_size) if batch_size is not None else int(cfg.micro_batch_size)\n",
        "\n",
        "    batch = get_batch(\n",
        "        batch_size=bs,\n",
        "        device=device,\n",
        "        vocab_n=cfg.vocab_n,\n",
        "        block_size=int(block_size),\n",
        "        allow_duplicates=cfg.allow_duplicates,\n",
        "    )\n",
        "\n",
        "    with (get_autocast_context(device, amp_dtype) if use_amp else nullcontext()):\n",
        "        logits, loss = model(batch, return_full_logits=False, block_size=int(block_size))\n",
        "\n",
        "    token_acc_t, sample_acc_t = acc_from_logits(logits, batch, int(block_size))\n",
        "    model.train()\n",
        "    return float(loss.item()), float(token_acc_t.item()), float(sample_acc_t.item())\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    vocab_n: int = 1024\n",
        "    block_size: int = 32\n",
        "    allow_duplicates: bool = True\n",
        "    sep_token: str = SEP_TOKEN\n",
        "\n",
        "    test_block_sizes: Optional[Tuple[int, ...]] = None\n",
        "\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 1\n",
        "    n_embd: int = 64\n",
        "    vocab_over_embd: Optional[float] = None\n",
        "\n",
        "    without_pos: bool = False\n",
        "    use_mlp: bool = True  # NEW: default uses MLPs\n",
        "\n",
        "    warmup_iters: int = 200\n",
        "    max_iters: int = 120000\n",
        "    learning_rate: float = 1e-4\n",
        "    min_lr: float = 1e-6\n",
        "    weight_decay: float = 0.0\n",
        "\n",
        "    micro_batch_size: int = 1024\n",
        "    effective_batch_size: int = 4096\n",
        "\n",
        "    log_interval: int = 250\n",
        "    ckpt_interval: int = 20000\n",
        "    save_dir: str = \"./saved_models\"\n",
        "\n",
        "    plateau_window_logs: int = 40\n",
        "    plateau_slope_threshold: float = 0.02\n",
        "    plateau_log10_loss_delta: float = 0.02\n",
        "    plateau_patience_logs: int = 2\n",
        "    plateau_extra_logs: int = 1\n",
        "    plateau_min_iters: int = 20000\n",
        "\n",
        "    seed: int = 1337\n",
        "    use_compile: bool = False\n",
        "\n",
        "    wandb_project: str = \"sortgpt\"\n",
        "    wandb_entity: Optional[str] = None\n",
        "    wandb_group: Optional[str] = None\n",
        "    wandb_mode: Optional[str] = None\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Naming / saving\n",
        "# -------------------------\n",
        "def make_wandb_run_name(cfg: TrainConfig) -> str:\n",
        "    rtag = _ratio_to_tag(cfg.vocab_over_embd)\n",
        "    test_ks = cfg.test_block_sizes if (cfg.test_block_sizes and len(cfg.test_block_sizes) > 0) else (cfg.block_size,)\n",
        "    ktag = _block_sizes_to_tag(tuple(int(k) for k in test_ks))\n",
        "    return (\n",
        "        f\"sortgpt\"\n",
        "        f\"_N{int(cfg.vocab_n)}\"\n",
        "        f\"_K{int(cfg.block_size)}\"\n",
        "        f\"_L{int(cfg.n_layers)}\"\n",
        "        f\"_npos{int(cfg.without_pos)}\"\n",
        "        f\"_mlp{int(cfg.use_mlp)}\"\n",
        "        f\"_r{rtag}\"\n",
        "        f\"_test{ktag}\"\n",
        "        f\"_dup{int(cfg.allow_duplicates)}\"\n",
        "    )\n",
        "\n",
        "def make_save_filename(prefix: str, cfg: TrainConfig, iters_done: int) -> str:\n",
        "    eff_n_embd = compute_effective_n_embd(cfg.vocab_n, cfg.n_heads, cfg.vocab_over_embd, cfg.n_embd)\n",
        "    test_ks = cfg.test_block_sizes if (cfg.test_block_sizes and len(cfg.test_block_sizes) > 0) else (cfg.block_size,)\n",
        "    test_ks = tuple(int(k) for k in test_ks)\n",
        "    rtag = _ratio_to_tag(cfg.vocab_over_embd)\n",
        "    ktag = _block_sizes_to_tag(test_ks)\n",
        "    return (\n",
        "        f\"{prefix}\"\n",
        "        f\"_N{int(cfg.vocab_n)}\"\n",
        "        f\"_K{int(cfg.block_size)}\"\n",
        "        f\"_L{int(cfg.n_layers)}\"\n",
        "        f\"_H{int(cfg.n_heads)}\"\n",
        "        f\"_E{int(eff_n_embd)}\"\n",
        "        f\"_r{rtag}\"\n",
        "        f\"_npos{int(cfg.without_pos)}\"\n",
        "        f\"_mlp{int(cfg.use_mlp)}\"\n",
        "        f\"_dup{int(cfg.allow_duplicates)}\"\n",
        "        f\"_test{ktag}\"\n",
        "        f\"_iters{int(iters_done)}.pt\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "def train_sorting_gpt(cfg: TrainConfig):\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "    # Default test sizes if not specified\n",
        "    test_block_sizes = tuple(int(k) for k in (cfg.test_block_sizes if cfg.test_block_sizes else (cfg.block_size,)))\n",
        "\n",
        "    # Validate no-duplicates across train + test K\n",
        "    if not cfg.allow_duplicates:\n",
        "        for k in (int(cfg.block_size),) + tuple(test_block_sizes):\n",
        "            if k > cfg.vocab_n:\n",
        "                raise ValueError(f\"allow_duplicates=False requires block_size <= vocab_n (got block_size={k}, vocab_n={cfg.vocab_n})\")\n",
        "        if cfg.block_size == cfg.vocab_n:\n",
        "            print(\"âš ï¸  Warning: block_size == vocab_n with allow_duplicates=False => degenerate constant sorted output.\")\n",
        "\n",
        "    total_vocab_size = cfg.vocab_n + 1\n",
        "    sep_id = cfg.vocab_n\n",
        "\n",
        "    grad_accum_steps = cfg.effective_batch_size // cfg.micro_batch_size\n",
        "    assert cfg.effective_batch_size % cfg.micro_batch_size == 0\n",
        "\n",
        "    # AMP settings\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    if use_amp:\n",
        "        bf16_ok = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
        "        amp_dtype = torch.bfloat16 if bf16_ok else torch.float16\n",
        "    else:\n",
        "        amp_dtype = None\n",
        "\n",
        "    scaler = make_grad_scaler(enabled=(use_amp and amp_dtype == torch.float16))\n",
        "\n",
        "    # Effective embd dim via ratio; max_seq_len for largest tested K\n",
        "    eff_n_embd = compute_effective_n_embd(cfg.vocab_n, cfg.n_heads, cfg.vocab_over_embd, cfg.n_embd)\n",
        "    max_k_for_model = max([int(cfg.block_size)] + [int(k) for k in test_block_sizes])\n",
        "    max_seq_len = 2 * max_k_for_model + 1\n",
        "\n",
        "    model_cfg = GPTConfig(\n",
        "        block_size=cfg.block_size,\n",
        "        vocab_size=total_vocab_size,\n",
        "        n_layers=cfg.n_layers,\n",
        "        n_heads=cfg.n_heads,\n",
        "        n_embd=eff_n_embd,\n",
        "        without_pos=cfg.without_pos,\n",
        "        use_mlp=cfg.use_mlp,\n",
        "        max_seq_len=max_seq_len,\n",
        "    )\n",
        "    model = GPT(model_cfg).to(device)\n",
        "\n",
        "    if cfg.use_compile and hasattr(torch, \"compile\"):\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"max-autotune\")\n",
        "            print(\"torch.compile enabled\")\n",
        "        except Exception as e:\n",
        "            print(f\"torch.compile failed, continuing uncompiled: {e}\")\n",
        "\n",
        "    optimizer = create_optimizer(model, weight_decay=cfg.weight_decay, lr=cfg.learning_rate)\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "\n",
        "    wandb_cfg = asdict(cfg)\n",
        "    wandb_cfg.update(\n",
        "        dict(\n",
        "            total_vocab_size=total_vocab_size,\n",
        "            sep_id=sep_id,\n",
        "            max_seq_len=max_seq_len,\n",
        "            max_k_for_model=max_k_for_model,\n",
        "            grad_accum_steps=grad_accum_steps,\n",
        "            amp_dtype=str(amp_dtype) if amp_dtype is not None else \"none\",\n",
        "            device=str(device),\n",
        "            test_block_sizes=list(test_block_sizes),\n",
        "            n_embd_effective=int(eff_n_embd),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=cfg.wandb_project,\n",
        "        entity=cfg.wandb_entity,\n",
        "        group=cfg.wandb_group,\n",
        "        name=make_wandb_run_name(cfg),\n",
        "        config=wandb_cfg,\n",
        "        mode=cfg.wandb_mode,\n",
        "    )\n",
        "\n",
        "    # Metrics\n",
        "    run.define_metric(\"iter\")\n",
        "    run.define_metric(\"train/*\", step_metric=\"iter\")\n",
        "    run.define_metric(\"test/*\", step_metric=\"iter\")\n",
        "    run.define_metric(\"lr\", step_metric=\"iter\")\n",
        "    run.define_metric(\"plateau/*\", step_metric=\"iter\")\n",
        "    run.define_metric(\"ll/iter\")\n",
        "    run.define_metric(\"ll/*\", step_metric=\"ll/iter\")\n",
        "\n",
        "    ll_iters_hist, ll_loss_hist = [], []\n",
        "    plateau_hits = 0\n",
        "    plateau_reached = False\n",
        "    plateau_reached_iter = None\n",
        "    stop_after_log_iter = None\n",
        "\n",
        "    last_logged_itr = None\n",
        "    iters_done = cfg.max_iters\n",
        "    last_log_t = time.time()\n",
        "\n",
        "    with get_sdpa_context():\n",
        "        for itr in trange(cfg.max_iters, desc=\"training\"):\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss_accum = torch.zeros((), device=device)\n",
        "\n",
        "            do_log = (itr % cfg.log_interval == 0)\n",
        "            if do_log:\n",
        "                token_correct = torch.zeros((), device=device)\n",
        "                sample_correct = torch.zeros((), device=device)\n",
        "                token_total = 0\n",
        "                sample_total = 0\n",
        "\n",
        "            for _ in range(grad_accum_steps):\n",
        "                batch = get_batch(\n",
        "                    batch_size=cfg.micro_batch_size,\n",
        "                    device=device,\n",
        "                    vocab_n=cfg.vocab_n,\n",
        "                    block_size=cfg.block_size,\n",
        "                    allow_duplicates=cfg.allow_duplicates,\n",
        "                )\n",
        "\n",
        "                if use_amp:\n",
        "                    with get_autocast_context(device, amp_dtype):\n",
        "                        logits, loss = model(batch, return_full_logits=False, block_size=cfg.block_size)\n",
        "                else:\n",
        "                    logits, loss = model(batch, return_full_logits=False, block_size=cfg.block_size)\n",
        "\n",
        "                # Train acc on the same microbatches at log steps\n",
        "                if do_log:\n",
        "                    with torch.no_grad():\n",
        "                        targets = batch[:, cfg.block_size + 1 :]\n",
        "                        preds = logits.detach().argmax(dim=-1)\n",
        "                        token_correct += (preds == targets).sum()\n",
        "                        sample_correct += (preds == targets).all(dim=1).sum()\n",
        "                        token_total += targets.numel()\n",
        "                        sample_total += targets.size(0)\n",
        "\n",
        "                loss_to_back = loss / grad_accum_steps\n",
        "                if scaler.is_enabled():\n",
        "                    scaler.scale(loss_to_back).backward()\n",
        "                else:\n",
        "                    loss_to_back.backward()\n",
        "\n",
        "                loss_accum += loss.detach()\n",
        "\n",
        "            lr = get_lr(itr, cfg)\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = lr\n",
        "\n",
        "            if scaler.is_enabled():\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "            # ---- log + plateau check ----\n",
        "            if do_log:\n",
        "                train_loss = float((loss_accum / grad_accum_steps).item())\n",
        "                train_token_acc = float((token_correct / max(token_total, 1)).item())\n",
        "                train_sample_acc = float((sample_correct / max(sample_total, 1)).item())\n",
        "\n",
        "                steps_done = itr + 1\n",
        "                ll_iter = _safe_log10(steps_done)\n",
        "                ll_loss = _safe_log10(train_loss)\n",
        "                ll_lr = _safe_log10(lr)\n",
        "\n",
        "                ll_iters_hist.append(ll_iter)\n",
        "                ll_loss_hist.append(ll_loss)\n",
        "\n",
        "                slope = None\n",
        "                delta_ll_loss = None\n",
        "                plateau_now = 0\n",
        "\n",
        "                if (not plateau_reached) and (len(ll_iters_hist) >= cfg.plateau_window_logs) and (steps_done >= cfg.plateau_min_iters):\n",
        "                    xs = ll_iters_hist[-cfg.plateau_window_logs:]\n",
        "                    ys = ll_loss_hist[-cfg.plateau_window_logs:]\n",
        "                    slope = _linreg_slope(xs, ys)\n",
        "                    delta_ll_loss = ys[0] - ys[-1]\n",
        "                    is_improving = (delta_ll_loss >= 0.0)\n",
        "                    plateau_now = int(\n",
        "                        is_improving and (delta_ll_loss < cfg.plateau_log10_loss_delta) and (slope > -cfg.plateau_slope_threshold)\n",
        "                    )\n",
        "                    plateau_hits = (plateau_hits + 1) if plateau_now else 0\n",
        "                    if plateau_hits >= cfg.plateau_patience_logs:\n",
        "                        plateau_reached = True\n",
        "                        plateau_reached_iter = itr\n",
        "                        stop_after_log_iter = itr + cfg.log_interval * max(int(cfg.plateau_extra_logs), 0)\n",
        "                        print(\n",
        "                            f\"ðŸŸ¨ plateau detected @ itr={itr} (slope={slope:.4f}, Î”log10(loss)={delta_ll_loss:.4f}); \"\n",
        "                            f\"will stop after logging itr={stop_after_log_iter}\"\n",
        "                        )\n",
        "\n",
        "                now = time.time()\n",
        "                dt = now - last_log_t\n",
        "                last_log_t = now\n",
        "                print(\n",
        "                    f\"itr: {itr} lr: {lr:.3e} train loss: {train_loss:.6f} \"\n",
        "                    f\"train token_acc: {train_token_acc:.4f} train sample_acc: {train_sample_acc:.4f} (dt={dt:.2f}s)\"\n",
        "                )\n",
        "\n",
        "                log_dict = {\n",
        "                    \"iter\": steps_done,\n",
        "                    \"train/loss\": train_loss,\n",
        "                    \"train/token_acc\": train_token_acc,\n",
        "                    \"train/sample_acc\": train_sample_acc,\n",
        "                    \"lr\": lr,\n",
        "                    \"ll/iter\": ll_iter,\n",
        "                    \"ll/train_loss\": ll_loss,\n",
        "                    \"ll/lr\": ll_lr,\n",
        "                    \"plateau/now\": plateau_now,\n",
        "                    \"plateau/hits\": plateau_hits,\n",
        "                    \"plateau/reached\": int(plateau_reached),\n",
        "                }\n",
        "                if slope is not None:\n",
        "                    log_dict[\"plateau/loglog_slope\"] = float(slope)\n",
        "                if delta_ll_loss is not None:\n",
        "                    log_dict[\"plateau/delta_log10_loss_window\"] = float(delta_ll_loss)\n",
        "\n",
        "                # Test metrics on each test block size\n",
        "                for k in test_block_sizes:\n",
        "                    test_loss, test_token_acc, test_sample_acc = eval_batch_metrics(\n",
        "                        model=model,\n",
        "                        device=device,\n",
        "                        cfg=cfg,\n",
        "                        block_size=int(k),\n",
        "                        use_amp=use_amp,\n",
        "                        amp_dtype=amp_dtype,\n",
        "                        batch_size=cfg.micro_batch_size,\n",
        "                    )\n",
        "                    log_dict[f\"test/K{int(k)}/loss\"] = test_loss\n",
        "                    log_dict[f\"test/K{int(k)}/token_acc\"] = test_token_acc\n",
        "                    log_dict[f\"test/K{int(k)}/sample_acc\"] = test_sample_acc\n",
        "\n",
        "                run.log(log_dict, step=steps_done)\n",
        "                last_logged_itr = itr\n",
        "\n",
        "                if plateau_reached and (stop_after_log_iter is not None) and (itr >= stop_after_log_iter):\n",
        "                    iters_done = steps_done\n",
        "                    break\n",
        "\n",
        "            # ---- checkpointing ----\n",
        "            if cfg.ckpt_interval and (itr > 0) and (itr % cfg.ckpt_interval == 0):\n",
        "                steps_done = itr + 1\n",
        "                ckpt_name = make_save_filename(\"Checkpoint\", cfg, steps_done)\n",
        "                ckpt_path = os.path.join(cfg.save_dir, ckpt_name)\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"itr\": itr,\n",
        "                        \"iters_done\": steps_done,\n",
        "                        \"plateau_reached\": plateau_reached,\n",
        "                        \"plateau_reached_iter\": plateau_reached_iter,\n",
        "                        \"train_config\": wandb_cfg,\n",
        "                        \"model_config\": vars(model_cfg),\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                    },\n",
        "                    ckpt_path,\n",
        "                )\n",
        "                print(f\"saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "    # Final save (filename includes grid params)\n",
        "    final_name = make_save_filename(\"Final\", cfg, iters_done)\n",
        "    final_path = os.path.join(cfg.save_dir, final_name)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"iters_done\": iters_done,\n",
        "            \"plateau_reached\": plateau_reached,\n",
        "            \"plateau_reached_iter\": plateau_reached_iter,\n",
        "            \"train_config\": wandb_cfg,\n",
        "            \"model_config\": vars(model_cfg),\n",
        "            \"model\": model.state_dict(),\n",
        "        },\n",
        "        final_path,\n",
        "    )\n",
        "    print(f\"saved final model: {final_path}\")\n",
        "\n",
        "    run.summary[\"iters_done\"] = iters_done\n",
        "    run.summary[\"plateau_reached\"] = bool(plateau_reached)\n",
        "    run.summary[\"plateau_reached_iter\"] = int(plateau_reached_iter) if plateau_reached_iter is not None else None\n",
        "    run.finish()\n",
        "    return final_path\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Grid runner (NOW: allow_duplicates + use_mlp are grid dimensions)\n",
        "# -------------------------\n",
        "def run_grid(\n",
        "    base_cfg: TrainConfig,\n",
        "    vocab_sizes: Iterable[int],\n",
        "    layer_counts: Iterable[int],\n",
        "    block_sizes: Iterable[int],\n",
        "    without_pos_flags: Iterable[bool],\n",
        "    vocab_over_embd_list: Iterable[float],\n",
        "    allow_duplicates_flags: Optional[Iterable[bool]] = None,  # NEW\n",
        "    use_mlp_flags: Optional[Iterable[bool]] = None,           # NEW\n",
        "):\n",
        "    vocab_sizes = sorted(list(vocab_sizes), reverse=True)\n",
        "    block_sizes = sorted(list(block_sizes), reverse=True)\n",
        "    layer_counts = sorted(list(layer_counts), reverse=True)\n",
        "    vocab_over_embd_list = sorted(list(vocab_over_embd_list))  # smaller => bigger embd => more expensive\n",
        "\n",
        "    if allow_duplicates_flags is None:\n",
        "        allow_duplicates_flags = [bool(base_cfg.allow_duplicates)]\n",
        "    else:\n",
        "        allow_duplicates_flags = list(allow_duplicates_flags)\n",
        "\n",
        "    if use_mlp_flags is None:\n",
        "        use_mlp_flags = [True]  # per your request: default is to use MLPs\n",
        "    else:\n",
        "        use_mlp_flags = list(use_mlp_flags)\n",
        "\n",
        "    # Default test block sizes = largest block size in the grid (same as your request)\n",
        "    if base_cfg.test_block_sizes is None or len(base_cfg.test_block_sizes) == 0:\n",
        "        default_test_block_sizes = (max(int(k) for k in block_sizes),)\n",
        "    else:\n",
        "        default_test_block_sizes = tuple(int(k) for k in base_cfg.test_block_sizes)\n",
        "\n",
        "    for N in vocab_sizes:\n",
        "        for K in block_sizes:\n",
        "            for L in layer_counts:\n",
        "                for npos in without_pos_flags:\n",
        "                    for mlp_on in use_mlp_flags:\n",
        "                        for dup in allow_duplicates_flags:\n",
        "                            for r in vocab_over_embd_list:\n",
        "                                cfg = copy.deepcopy(base_cfg)\n",
        "                                cfg.vocab_n = int(N)\n",
        "                                cfg.block_size = int(K)\n",
        "                                cfg.n_layers = int(L)\n",
        "                                cfg.without_pos = bool(npos)\n",
        "                                cfg.use_mlp = bool(mlp_on)\n",
        "                                cfg.allow_duplicates = bool(dup)\n",
        "                                cfg.vocab_over_embd = float(r)\n",
        "\n",
        "                                cfg.test_block_sizes = default_test_block_sizes\n",
        "                                train_sorting_gpt(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3c1TPg8nmo9"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "PROJECT = globals().get(\"PROJECT\", \"sortgpt\")\n",
        "GROUP = f\"grid_{STAMP}\"\n",
        "\n",
        "GRID_ROOT = f\"./grid_outputs_{GROUP}\"\n",
        "WANDB_DIR = os.path.join(GRID_ROOT, \"wandb\")\n",
        "SAVE_DIR  = os.path.join(GRID_ROOT, \"saved_models\")\n",
        "\n",
        "os.makedirs(WANDB_DIR, exist_ok=True)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Make sure ALL wandb local artifacts go here\n",
        "os.environ[\"WANDB_DIR\"] = WANDB_DIR\n",
        "os.environ[\"WANDB_CACHE_DIR\"] = os.path.join(GRID_ROOT, \"wandb_cache\")\n",
        "os.environ[\"WANDB_CONFIG_DIR\"] = os.path.join(GRID_ROOT, \"wandb_config\")\n",
        "\n",
        "print(\"GRID_ROOT =\", GRID_ROOT)\n",
        "print(\"WANDB_DIR =\", WANDB_DIR)\n",
        "print(\"SAVE_DIR  =\", SAVE_DIR)\n",
        "print(\"PROJECT   =\", PROJECT)\n",
        "print(\"GROUP     =\", GROUP)\n",
        "\n",
        "# Optional: nicer run name including all grid params\n",
        "def make_wandb_run_name(cfg: TrainConfig) -> str:\n",
        "    ga = int(cfg.effective_batch_size) // int(cfg.micro_batch_size)\n",
        "    rtag = _ratio_to_tag(cfg.vocab_over_embd)\n",
        "    test_ks = cfg.test_block_sizes if (cfg.test_block_sizes and len(cfg.test_block_sizes) > 0) else (cfg.block_size,)\n",
        "    ktag = _block_sizes_to_tag(tuple(int(k) for k in test_ks))\n",
        "    return (\n",
        "        f\"bs{int(cfg.micro_batch_size)}\"\n",
        "        f\"_eb{int(cfg.effective_batch_size)}\"\n",
        "        f\"_ga{ga}\"\n",
        "        f\"_N{int(cfg.vocab_n)}\"\n",
        "        f\"_K{int(cfg.block_size)}\"\n",
        "        f\"_L{int(cfg.n_layers)}\"\n",
        "        f\"_npos{int(cfg.without_pos)}\"\n",
        "        f\"_mlp{int(cfg.use_mlp)}\"\n",
        "        f\"_r{rtag}\"\n",
        "        f\"_test{ktag}\"\n",
        "        f\"_dup{int(cfg.allow_duplicates)}\"\n",
        "    )\n",
        "\n",
        "base_cfg = TrainConfig(\n",
        "    vocab_n=256,              # overridden by grid\n",
        "    block_size=16,            # overridden by grid\n",
        "    allow_duplicates=False,   # overridden via allow_duplicates_flags in run_grid\n",
        "\n",
        "    n_layers=2,               # overridden by grid\n",
        "    n_heads=1,\n",
        "\n",
        "    n_embd=64,                # used only if vocab_over_embd is None (grid sets vocab_over_embd)\n",
        "    vocab_over_embd=None,\n",
        "\n",
        "    without_pos=False,        # overridden by grid\n",
        "    use_mlp=True,             # default True (can be overridden via use_mlp_flags)\n",
        "    test_block_sizes=None,\n",
        "\n",
        "    max_iters=60000,\n",
        "    warmup_iters=200,\n",
        "    learning_rate=1e-4,\n",
        "    min_lr=1e-6,\n",
        "    weight_decay=0.0,\n",
        "\n",
        "    micro_batch_size=4096,\n",
        "    effective_batch_size=4096,\n",
        "\n",
        "    log_interval=250,\n",
        "    ckpt_interval=20000,\n",
        "    save_dir=SAVE_DIR,\n",
        "\n",
        "    plateau_window_logs=40,\n",
        "    plateau_min_iters=20000,\n",
        "    plateau_slope_threshold=0.02,\n",
        "    plateau_log10_loss_delta=0.02,\n",
        "    plateau_patience_logs=2,\n",
        "    plateau_extra_logs=1,\n",
        "\n",
        "    wandb_project=PROJECT,\n",
        "    wandb_group=GROUP,\n",
        ")\n",
        "\n",
        "run_grid(\n",
        "    base_cfg,\n",
        "    vocab_sizes=[128, 64],\n",
        "    layer_counts=[1, 2, 3],\n",
        "    block_sizes=[16],\n",
        "    without_pos_flags=[False, True],\n",
        "    vocab_over_embd_list=[2, 4, 8],\n",
        "    allow_duplicates_flags=[False],\n",
        "    use_mlp_flags=[True],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
